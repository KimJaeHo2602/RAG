{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë””ë ‰í† ë¦¬ ë¡œë”\n",
    "\n",
    "- ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ì„ í¬í•¨í•˜ì—¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ë¡œë“œí•˜ëŠ” ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-magic-bin\n",
    "#pip install limagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The MIME type of '..\\\\data\\\\SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf' is \"cannot open `..\\\\data\\\\SPRI_AI_Brief_2023\\\\353\\\\205\\\\20412\\\\354\\\\233\\\\224\\\\355\\\\230\\\\270_F.pdf' (Illegal byte sequence)\". This file type is not currently supported in unstructured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ë¡œë” ì´ˆê¸°í™”\n",
    "loader = DirectoryLoader(\"../data\", glob=\"**/*.pdf\")\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "docs = loader.load()\n",
    "# ë¬¸ì„œ ê°œìˆ˜ ê³„ì‚°\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loader_cls ë³€ê²½\n",
    "\n",
    "loader_cls ëŠ” ê¸°ë³¸ ê°’ìœ¼ë¡œ UnstructuredLoader í´ëž˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¡œë”ë¥¼ ì‚¬ìš©ìž ì •ì˜í•˜ë ¤ë©´ loader_cls kwargì— ë¡œë” í´ëž˜ìŠ¤ë¥¼ ì§€ì •í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# loader_cls ë¥¼ TextLoader ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.\n",
    "loader = DirectoryLoader(\"../\", glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\5.LlamaParseLoader.py'}, page_content='import os\\nfrom typing import Iterator\\nfrom typing import Any, Dict, List, Optional\\nfrom langchain_core.document_loaders import BaseLoader\\nfrom langchain_core.documents import Document\\nfrom llama_parse import LlamaParse\\nfrom llama_index.core import SimpleDirectoryReader\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n\\nclass LlamaParseLoader(BaseLoader):\\n    \"\"\"íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ì˜¤ëŠ” ë¬¸ì„œ ë¡œë”ì˜ ì˜ˆì‹œìž…ë‹ˆë‹¤.\"\"\"\\n\\n    def __init__(self, file_paths: List[str], parsing_instructions=\"\") -> None:\\n        \"\"\"ë¡œë”ë¥¼ íŒŒì¼ ê²½ë¡œì™€ í•¨ê»˜ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\\n        Args:\\n            file_paths: ë¡œë“œí•  íŒŒì¼ì˜ ê²½ë¡œìž…ë‹ˆë‹¤.\\n        \"\"\"\\n        # LlamaParse ì„¤ì •\\n        parser = LlamaParse(\\n            # api_key=\"llx-...\",  # API í‚¤ (í™˜ê²½ ë³€ìˆ˜ LLAMA_CLOUD_API_KEYì— ì €ìž¥ ê°€ëŠ¥)\\n            result_type=\"markdown\",  # ê²°ê³¼ íƒ€ìž…: \"markdown\" ë˜ëŠ” \"text\"\\n            num_workers=4,  # ì—¬ëŸ¬ íŒŒì¼ ì²˜ë¦¬ ì‹œ API í˜¸ì¶œ ë¶„í•  ìˆ˜\\n            verbose=True,\\n            language=\"ko\",  # ì–¸ì–´ ì„¤ì • (ê¸°ë³¸ê°’: \\'en\\')\\n            invalidate_cache=True,\\n            skip_diagonal_text=True,\\n            use_vendor_multimodal_model=True,\\n            vendor_multimodal_model_name=\"openai-gpt4o\",\\n            vendor_multimodal_api_key=os.environ.get(\"OPENAI_API_KEY\"),\\n            parsing_instruction=parsing_instructions,\\n        )\\n\\n        file_extractor = {\".pdf\": parser}\\n\\n        self.document_reader = SimpleDirectoryReader(\\n            input_files=file_paths,\\n            file_extractor=file_extractor,\\n        )\\n\\n    def lazy_load(self) -> Iterator[Document]:  # <-- ì¸ìžë¥¼ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤\\n        \"\"\"íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ì˜¤ëŠ” ì§€ì—° ë¡œë”ìž…ë‹ˆë‹¤.\\n\\n        ì§€ì—° ë¡œë“œ ë©”ì†Œë“œë¥¼ êµ¬í˜„í•  ë•ŒëŠ”, ë¬¸ì„œë¥¼ í•˜ë‚˜ì”© ìƒì„±í•˜ì—¬ ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\\n        \"\"\"\\n        documents = self.document_reader.load_data()\\n        langchain_documents = [doc.to_langchain_format() for doc in documents]\\n        return langchain_documents\\n'),\n",
       " Document(metadata={'source': '..\\\\streamlit_practice.py'}, page_content='import streamlit as st\\n\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain import hub\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\nst.title(\"ë°©ì´ì˜ chat gpt test\")\\n\\n# messageê°€ ì—†ìœ¼ë©´ ë¹ˆ list ìƒì„±\\nif \"messages\" not in st.sesstion_state:\\n    st.session_state[\"messages\"] = []\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"ëŒ€í™”ì´ˆê¸°í™”\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\01-ChaptGPT\\\\main.py'}, page_content='import streamlit as st\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\n\\nif not os.path.exists(\"cache\"):\\n    os.mkdir(\"cache\")\\n\\nif not os.path.exists(\"cache/files\"):\\n    os.mkdir(\"cache/fiels\")\\n\\nif not os.path.exists(\"cache/embeddings\"):\\n    os.mkdir(\"cache/embeddings\")\\n\\n\\n# ëŒ€í™” ê¸°ë¡ ì €ìž¥ì„ ìœ„í•œ session_state\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_messages(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\nst.title(\"pdfê¸°ë°˜ Q&AðŸ’¬\")\\n\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"íŒŒì¼ ì—…ë¡œë“œ\", tpye=\"pdf\")\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"ëª¨ë¸ì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ë° ì¸ë±ì‹±\\n@st.cache_resource(show_spinner=\"íŒŒì¼ì´ ì—…ë¡œë“œì¤‘ìž…ë‹ˆë‹¤\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# Create Chain\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name)\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n#######---------------------\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ë©´\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    # chain ê°ì²´ë¥¼ st.session_state[\"chain\"]ì— ì €ìž¥\\n    st.session_state[\"chain\"] = chain\\n\\n# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ì„ ëˆ„ë¥´ë©´\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ìœ„í•¨\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    # ì²´ì¸ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”\")\\n\\n\\n# ##############\\n# if uploaded_file:\\n#     retriever = embed_file(uploaded_file)\\n#     chain = create_chain(retriever, model=selected_model)\\n#     st.session_state[\"messages\"] = chain\\n\\n# if clear_btn:\\n#     st.session_state[\"message\"] = []\\n\\n# print_messages()\\n\\n# user_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# warning_msg = st.emplty()\\n\\n# if user_input:\\n#     chain = st.session_state[\"chain\"]\\n\\n#     if chain is not None:\\n#         # ì‚¬ìš©ìž ìž…ë ¥\\n#         st.chat_message(\"user\").write(user_input)\\n#         # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n#         response = chain.stream(user_input)\\n\\n#         with st.chat_message(\"assistant\"):\\n#             container = st.empty()\\n\\n#             ai_answer = \"\"\\n#             for token in response:\\n#                 ai_answer += token\\n#                 container.markdown(ai_answer)\\n\\n#         add_messages(\"user\", user_input)\\n#         add_messages(\"assistant\", ai_answer)\\n#     else:\\n#         warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\")\\n# ##############\\n\\n# if uploaded_file:\\n#     retriever = embed_file(uploaded_file)\\n#     chain = create_chain(retriever, model=selected_model)\\n#     st.session_state[\"messages\"] = chain\\n\\n# if clear_btn:\\n#     st.session_state[\"meesages\"] = []\\n\\n# print_messages()\\n\\n# user_input = st.chat_input(\"ê¶ê¸ˆí•˜ ë‚´ìš©\")\\n# warning_msg = st.empty()\\n\\n# if user_input:\\n#     chain = st.session_state(\"chain\")\\n#     if chain is not None:\\n#         st.chat_message(\"user\").write(user_input)\\n\\n#         response = chain.stream(\"user_input\")\\n\\n#         with st.chat_message(\"assistant\"):\\n#             container = st.empty()\\n\\n#             ai_answer = \"\"\\n#             for token in response:\\n#                 ai_answer += token\\n#                 container.markdown(ai_answer)\\n\\n#         add_messages(\"user\", user_input)\\n#         add_messages(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\01-ChaptGPT\\\\new_main.py'}, page_content='import streamlit as st\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import load_prompt\\n\\n\\nst.set_page_config(page_title=\"ë‚˜ë§Œì˜ ChatGPT ðŸ’¬\", page_icon=\"ðŸ’¬\")\\nst.title(\"ë‚˜ë§Œì˜ ChatGPT ðŸ’¬\")\\n\\n# st.session_stateëŠ” Streamlitì—ì„œ ì„¸ì…˜ë³„ë¡œ ìƒíƒœë¥¼ ê´€ë¦¬í•  ìˆ˜ ìžˆë„ë¡ í•˜ëŠ” ê°ì²´ìž…ë‹ˆë‹¤.  session_stateëŠ” ì„¸ì…˜ë³„ë¡œ ë°ì´í„°ë¥¼ ìœ ì§€í•œë‹¤.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n\\n# ëª¨ë“  ì±„íŒ…ì˜ ížˆìŠ¤í† ë¦¬ë¥¼ í™”ë©´ì— ì¶œë ¥. st.chat_messageëŠ” Streamlitì—ì„œ ì±„íŒ… í˜•ì‹ì˜ ë©”ì‹œì§€ë¥¼ í™”ë©´ì— ì¶œë ¥í•˜ëŠ” ë° ì‚¬ìš©\\ndef print_history():\\n    for msg in st.session_state[\"messages\"]:\\n        st.chat_message(msg.role).write(msg.content)\\n\\n\\n# ìƒˆë¡œìš´ ì±„íŒ… ë©”ì„¸ì§€ë¥¼ st.session_state[\\'messages]ì— ì¶”ê°€\\ndef add_history(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\n# ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\\ndef create_chain(prompt, model):\\n    chain = prompt | ChatOpenAI(model_name=model) | StrOutputParser()\\n    return chain\\n\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”\")\\n    tab1, tab2 = st.tabs([\"í”„ë¡¬í”„íŠ¸\", \"í”„ë¦¬ì…‹\"])\\n    prompt = \"\"\"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\"\"\\n    user_text_prompt = tab1.text_area(\"í”„ë¡¬í”„íŠ¸\", value=prompt)\\n    user_text_apply_btn = tab1.button(\"í”„ë¡¬í”„íŠ¸ ì ìš©\", key=\"apply1\")\\n    if user_text_apply_btn:\\n        tab1.markdown(f\"âœ… í”„ë¡¬í”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤\")\\n        prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n        prompt = PromptTemplate.from_template(prompt_template)\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-4o-mini\")\\n\\n    user_selected_prompt = tab2.selectbox(\"í”„ë¦¬ì…‹ ì„ íƒ\", [\"sns\", \"ë²ˆì—­\", \"ìš”ì•½\"])\\n    user_selected_apply_btn = tab2.button(\"í”„ë¡¬í”„íŠ¸ ì ìš©\", key=\"apply2\")\\n    if user_selected_apply_btn:\\n        tab2.markdown(f\"âœ… í”„ë¡¬í”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤\")\\n        prompt = load_prompt(f\"prompts/{user_selected_prompt}.yaml\", encoding=\"utf8\")\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-4o-mini\")\\n\\nif clear_btn:\\n    retriever = st.session_state[\"messages\"].clear()\\n\\nprint_history()\\n\\n\\nif \"chain\" not in st.session_state:\\n    # user_prompt\\n    prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n    prompt = PromptTemplate.from_template(prompt_template)\\n    st.session_state[\"chain\"] = create_chain(prompt, \"gpt-4o-mini\")\\n\\nif user_input := st.chat_input():\\n    add_history(\"user\", user_input)\\n    st.chat_message(\"user\").write(user_input)\\n    with st.chat_message(\"assistant\"):\\n        chat_container = st.empty()\\n\\n        stream_response = st.session_state[\"chain\"].stream(\\n            {\"question\": user_input}\\n        )  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜\\n        ai_answer = \"\"\\n        for chunk in stream_response:\\n            ai_answer += chunk\\n            chat_container.markdown(ai_answer)\\n        add_history(\"ai\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\02-ì´ë©”ì¼ í”„ë¡œì íŠ¸-PudanticOuptputparserâ™¥\\\\main.py'}, page_content='import os\\nfrom dotenv import load_dotenv\\nimport streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_core.output_parsers import PydanticOutputParser\\nfrom langchain_community.utilities import SerpAPIWrapper\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n# ê²€ìƒ‰ì„ ìœ„í•œ api_key\\nos.environ[\"SERPAPI_API_KEY\"] = (\\n    \"9020cdf9235a65b087f324a802b283ff67ac56cf8ddf0ab5bc8522c284d4a573\"\\n)\\n\\nst.title(\"Email ìš”ì•½ê¸°ðŸ’¬\")\\n\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# pydantic. ì´ë©”ì¼ ë³¸ë¬¸ì—ì„œ ì£¼ìš” ì—”í‹°í‹° ì¶”ì¶œ\\nclass EmailSummary(BaseModel):\\n    person: str = Field(description=\"ë©”ì¼ì„ ë³´ë‚¸ ì‚¬ëžŒ\")\\n    email: str = Field(description=\"ë©”ì¼ì„ ë³´ë‚¸ ì‚¬ëžŒì˜ ì´ë©”ì¼ ì£¼ì†Œ\")\\n    subject: str = Field(description=\"ë©”ì¼ ì œëª©\")\\n    summary: str = Field(description=\"ë©”ì¼ ë³¸ë¬¸ì„ ìš”ì•½í•œ í…ìŠ¤íŠ¸\")\\n    date: str = Field(description=\"ë©”ì¼ ë³¸ë¬¸ì— ì–¸ê¸‰ëœ ë¯¸íŒ… ë‚ ì§œì™€ ì‹œê°„\")\\n    company: str = Field(description=\"ë©”ì¼ì„ ë³´ë‚¸ ì‚¬ëžŒì˜ íšŒì‚¬ì •ë³´\")\\n\\n\\n# pydantic ì²´ì¸ ìƒì„±\\ndef create_email_parsing_chain():\\n\\n    llm = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\n    # PydanticOutputParser ìƒì„± - promptì— get_format_instructionì„ ë„£ì–´ì£¼ê¸° ìœ„í•´ì„œ.\\n    output_parser = PydanticOutputParser(pydantic_object=EmailSummary)\\n\\n    # PROMPT ìƒì„±\\n    template = \"\"\"\\n    You are a helpful assistant. Please answer the following questions in KOREAN.\\n\\n    #Question:\\n    ë‹¤ìŒì˜ ì´ë©”ì¼ ë‚´ìš© ì¤‘ì—ì„œ ì£¼ìš” ë‚´ìš©ì„ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.\\n\\n    #Email Converation:\\n    {email_conversation}\\n\\n    #Format:\\n    {format}\\n    \"\"\"\\n\\n    prompt = PromptTemplate.from_template(template)\\n    prompt = prompt.partial(format=output_parser.get_format_instructions())\\n\\n    chain = prompt | llm | output_parser\\n\\n    return chain\\n\\n\\n# ë‘ë²ˆì§¸ ì²´ì¸\\ndef create_report_chain():\\n\\n    llm = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\n    prompt = load_prompt(\"prompts/email.yaml\", encoding=\"utf-8\")\\n\\n    # ì¶œë ¥ íŒŒì„œ\\n    output_parser = StrOutputParser()\\n\\n    # ì²´ì¸ ìƒì„±\\n    chain = prompt | llm | output_parser\\n    return chain\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n    st.chat_message(\"user\").write(user_input)\\n\\n    # 1)emailì„ íŒŒì‹±í•˜ëŠ” chain ì„ ìƒì„± ë° ì‹¤í–‰\\n    email_chain = create_email_parsing_chain()\\n    # emailì—ì„œ ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì²´ì¸\\n    answer = email_chain.invoke({\"email_conversation\": user_input})\\n\\n    # 2) ë³´ë‚¸ ì‚¬ëžŒì˜ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘(ê²€ìƒ‰)\\n    params = {\\n        \"engine\": \"google\",\\n        \"gl\": \"kr\",\\n        \"hl\": \"ko\",\\n        \"num\": \"3\",\\n    }  # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° hlì€ google ui language, glì€ country\\n    search = SerpAPIWrapper(params=params)  # ê²€ìƒ‰ ê°ì²´\\n    search_query = f\"{answer.person} {answer.company} {answer.email}\"  # ê²€ìƒ‰ query\\n    search_result = search.run(search_query)  # ê²€ìƒ‰ queryì‹¤í–‰\\n    search_result = eval(search_result)  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜\\n    search_result_string = \"\\\\n\".join(\\n        search_result\\n    )  # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë§ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.\\n\\n    # 3) ì´ë©”ì¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±. ê²€ìƒ‰í•œ ê²°ê³¼ë¥¼ ê°€ì§€ê³  ì •ë¦¬ë¥¼ í•´ì¤€ë‹¤.\\n    report_chain = create_report_chain()\\n    report_chain_input = {\\n        \"sender\": answer.person,\\n        \"additional_information\": search_result_string,\\n        \"company\": answer.company,\\n        \"email\": answer.email,\\n        \"subject\": answer.subject,\\n        \"summary\": answer.summary,\\n        \"date\": answer.date,\\n    }\\n\\n    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n    response = report_chain.stream(report_chain_input)\\n    with st.chat_message(\"assistant\"):\\n        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n\\n# streamlit run .\\\\main.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\main.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom dotenv import load_dotenv\\nimport glob\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n\\nst.title(\"ë‚˜ë§Œì˜ ì±—GPTðŸ’¬\")\\n\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    prompt_files = glob.glob(\"prompts/*.yaml\")\\n    selected_prompt = st.selectbox(\"í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”\", prompt_files, index=0)\\n    task_input = st.text_input(\"TASK ìž…ë ¥\", \"\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(prompt_filepath, task=\"\"):\\n    # prompt ì ìš©\\n    prompt = load_prompt(prompt_filepath, encoding=\"utf-8\")\\n\\n    # ì¶”ê°€ íŒŒë¼ë¯¸í„°ê°€ ìžˆìœ¼ë©´ ì¶”ê°€\\n    if task:\\n        prompt = prompt.partial(task=task)\\n\\n    # GPT\\n    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\\n\\n    # ì¶œë ¥ íŒŒì„œ\\n    output_parser = StrOutputParser()\\n\\n    # ì²´ì¸ ìƒì„±\\n    chain = prompt | llm | output_parser\\n    return chain\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n    st.chat_message(\"user\").write(user_input)\\n    # chain ì„ ìƒì„±\\n    chain = create_chain(selected_prompt, task=task_input)\\n\\n    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):\\n        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\retriever.py'}, page_content='from langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\n\\n\\n# Indexing\\ndef create_retriever(file_path):\\n    # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í• \\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # 3ë‹¨ê³„: ìž„ë² ë”© ìƒì„±\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5ë‹¨ê³„: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\01_Multi_PDF copy.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDFê¸°ë°˜ QAðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ. - ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ì½”ë“œë¡œ ìƒì„±í•œë‹¤.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•ŠëŠ”ê²½ìš°\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"PDFíŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"modelì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¸Œë ¥ - chat_messageì—ëŠ” roleê³¼ contentê°€ ë“¤ì–´ê°€ ìžˆë‹¤.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)  \\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€ - ChatMessageì€ langchain_core.messages.chatì— ìžˆë‹¤.\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------------------------ indexing\\n    # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í• \\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # 3ë‹¨ê³„: ìž„ë² ë”© ìƒì„±\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5ë‹¨ê³„: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    # 7ë‹¨ê³„ ì–¸ì–´ëª¨ë¸ ìƒì„±\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8ë‹¨ê³„ ì²´ì¸ ìƒì„±\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ\\nif uploaded_file:\\n    # íŒŒì¼ ì—…ë¡œë“œ í›„ retrieverìƒì„±. ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦´ ì˜ˆì •\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\n# ë§Œì•½ ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´,\\nif user_input:\\n    # ì²´ì¸ ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë°\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        # íŒŒì¼ì„ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì„¸ì§€\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”\")\\n\\n# poetry shellë¡œ ê°€ìƒí™˜ê²½ ì‹¤í–‰\\n# streamlit run PDF.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\01_Multi_PDF.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDFê¸°ë°˜ QAðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ. - ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ì½”ë“œë¡œ ìƒì„±í•œë‹¤.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•ŠëŠ”ê²½ìš°\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_files = st.file_uploader(\\n        \"PDFíŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"], accept_multiple_files=True\\n    )\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"modelì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¸Œë ¥ - chat_messageì—ëŠ” roleê³¼ contentê°€ ë“¤ì–´ê°€ ìžˆë‹¤.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€ - ChatMessageì€ langchain_core.messages.chatì— ìžˆë‹¤.\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\ndef embed_files(files):\\n\\n    all_documents = []\\n\\n    for file in files:\\n        # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥\\n        file_content = file.read()\\n        file_path = f\"./.cache/files/{file.name}\"\\n        with open(file_path, \"wb\") as f:\\n            f.write(file_content)\\n\\n        # ------------------------------ indexing\\n        # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ\\n        loader = PDFPlumberLoader(file_path)\\n        docs = loader.load()\\n\\n        # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í• \\n        text_splitter = RecursiveCharacterTextSplitter(\\n            chunk_size=2000, chunk_overlap=100\\n        )\\n        documents = text_splitter.split_documents(docs)\\n\\n        all_documents.extend(documents)\\n\\n    if not all_documents:\\n        st.error(\"PDF íŒŒì¼ì—ì„œ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\\n        return None\\n\\n    # 3ë‹¨ê³„: ìž„ë² ë”© ìƒì„±\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\\n    vector_store = FAISS.from_documents(all_documents, embeddings)\\n\\n    # 5ë‹¨ê³„: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    # 7ë‹¨ê³„ ì–¸ì–´ëª¨ë¸ ìƒì„±\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8ë‹¨ê³„ ì²´ì¸ ìƒì„±\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ\\nif uploaded_files:\\n    # íŒŒì¼ ì—…ë¡œë“œ í›„ retrieverìƒì„±. ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦´ ì˜ˆì •\\n    retriever = embed_files(uploaded_files)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\n# ë§Œì•½ ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´,\\nif user_input:\\n    # ì²´ì¸ ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë°\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        # íŒŒì¼ì„ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì„¸ì§€\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”\")\\n\\n# poetry shellë¡œ ê°€ìƒí™˜ê²½ ì‹¤í–‰\\n# streamlit run PDF.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\01_PDF pratice.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_core.messages import ChatMessage\\n\\n\\nload_dotenv()\\n\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\"./cache\")\\n\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\"./cache/files\")\\n\\nif not os.path.exists(\".embeddings\"):\\n    os.mkdir(\"./embeddings\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\ndef add_message(role, message):\\n    st.session_state[\"message\"].append(ChatMessage(role=role, content=message))\\n\\n@st.cache_resource(show_spinner= \"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬í•´ ì£¼ì„¸ìš”\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\".cache/files/file\"'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\01_PDF.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDFê¸°ë°˜ QAðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ. - ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ì½”ë“œë¡œ ìƒì„±í•œë‹¤.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•ŠëŠ”ê²½ìš°\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"PDFíŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"modelì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¸Œë ¥ - chat_messageì—ëŠ” roleê³¼ contentê°€ ë“¤ì–´ê°€ ìžˆë‹¤.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€ - ChatMessageì€ langchain_core.messages.chatì— ìžˆë‹¤.\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------------------------ indexing\\n    # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í• \\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # 3ë‹¨ê³„: ìž„ë² ë”© ìƒì„±\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5ë‹¨ê³„: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    # 7ë‹¨ê³„ ì–¸ì–´ëª¨ë¸ ìƒì„±\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8ë‹¨ê³„ ì²´ì¸ ìƒì„±\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ\\nif uploaded_file:\\n    # íŒŒì¼ ì—…ë¡œë“œ í›„ retrieverìƒì„±. ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦´ ì˜ˆì •\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\n# ë§Œì•½ ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´,\\nif user_input:\\n    # ì²´ì¸ ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë°\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        # íŒŒì¼ì„ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì„¸ì§€\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”\")\\n\\n# poetry shellë¡œ ê°€ìƒí™˜ê²½ ì‹¤í–‰\\n# streamlit run PDF.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\01_PDF_extract table.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import (\\n    RecursiveCharacterTextSplitter,\\n    MarkdownTextSplitter,\\n)\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDFê¸°ë°˜ QAðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ. - ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ì½”ë“œë¡œ ìƒì„±í•œë‹¤.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•ŠëŠ”ê²½ìš°\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"PDFíŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"modelì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¸Œë ¥ - chat_messageì—ëŠ” roleê³¼ contentê°€ ë“¤ì–´ê°€ ìžˆë‹¤.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€ - ChatMessageì€ langchain_core.messages.chatì— ìžˆë‹¤.\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------------------------ indexing\\n    # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í•  - 21 Markdown\\n    markdown_splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    markdown_docs = markdown_splitter.split_documents(docs)\\n\\n    # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í•  - 2 Recursive\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(markdown_docs)\\n\\n    # 3ë‹¨ê³„: ìž„ë² ë”© ìƒì„±\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5ë‹¨ê³„: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    # 7ë‹¨ê³„ ì–¸ì–´ëª¨ë¸ ìƒì„±\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8ë‹¨ê³„ ì²´ì¸ ìƒì„±\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ\\nif uploaded_file:\\n    # íŒŒì¼ ì—…ë¡œë“œ í›„ retrieverìƒì„±. ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦´ ì˜ˆì •\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\n# ë§Œì•½ ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´,\\nif user_input:\\n    # ì²´ì¸ ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë°\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        # íŒŒì¼ì„ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì„¸ì§€\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”\")\\n\\n# poetry shellë¡œ ê°€ìƒí™˜ê²½ ì‹¤í–‰\\n# streamlit run PDF.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\02_Local_RAG.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom retriever import create_retriever\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"Local model ê¸°ë°˜ RAGðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ. - ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ì½”ë“œë¡œ ìƒì„±í•œë‹¤.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•ŠëŠ”ê²½ìš°\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"PDFíŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"modelì„ íƒ\", [\"gpt-4o-mini\", \"Ollama-EEVE\", \"Ollama-Llama3.1\"]\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¸Œë ¥ - chat_messageì—ëŠ” roleê³¼ contentê°€ ë“¤ì–´ê°€ ìžˆë‹¤.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€ - ChatMessageì€ langchain_core.messages.chatì— ìžˆë‹¤.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # create_retrieverë¥¼ retrieverë¡œë¶€í„° ê°€ì ¸ì˜¨ë‹¤.\\n    return create_retriever(file_path)\\n\\n\\n# retrieverí• ë•Œ, ë©”íƒ€ë°ì´í„°ë¥¼ì œì™¸í•˜ê³ , page_contentë§Œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•¨ì´ë‹¤. meta dataë¥¼ ì´í•´ ëª»í•˜ëŠ” ê²½ìš°ê°€ ìžˆê¸° ë•Œë¬¸ì´ë‹¤.\\ndef format_doc(document_list):\\n    return \"\\\\n\\\\n\".join([doc.page_content for doc in document_list])\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # ëª¨ë¸ ì´ë¦„ì´ gpt-4o-minië©´,\\n    if model_name == \"gpt-4o-mini\":\\n        # 6ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±\\n        prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n        # 7ë‹¨ê³„ ì–¸ì–´ëª¨ë¸ ìƒì„±\\n        llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # xionic = ChatOpenAI(\\n    #     model_name=\"xionic-1-72b-20240610\",\\n    #     base_url=\"https://sionic.chat/v1/\",\\n    #     api_key=\"934c4bbc-c384-4bea-af82-1450d7f8128d\",\\n    # )\\n    # pdf-ragë¡œ ì¨ë„ìž˜ë˜ë„¤?\\n    elif model_name == \"Ollama-EEVE\":\\n        prompt = load_prompt(\"prompts/pdf-rag-ollama-EEVE.yaml\", encoding=\"utf-8\")\\n        llm = ChatOllama(model=\"EEVE-Korean-10.8b:latest\", temperature=0)\\n\\n    elif model_name == \"Ollama-Llama3.1\":\\n        prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n        llm = ChatOllama(model=\"llama3.1\", temperature=0)\\n\\n    # 8ë‹¨ê³„ ì²´ì¸ ìƒì„±\\n    chain = (\\n        {\"context\": retriever | format_doc, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ\\nif uploaded_file:\\n    # íŒŒì¼ ì—…ë¡œë“œ í›„ retrieverìƒì„±. ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦´ ì˜ˆì •\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ëœ¨ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\n# ë§Œì•½ ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´,\\nif user_input:\\n    # ì²´ì¸ ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë°\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n\\n    else:\\n        # íŒŒì¼ì„ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì„¸ì§€\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”\")\\n\\n# poetry shellë¡œ ê°€ìƒí™˜ê²½ ì‹¤í–‰\\n# streamlit run .\\\\02.Local_lag.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\03_Multi_Modal.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_teddynote.models import MultiModal\\nfrom langchain_teddynote.messages import stream_response\\n\\nfrom retriever import create_retriever\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] ì´ë¯¸ì§€ ì¸ì‹\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"ì´ë¯¸ì§€ ì¸ì‹ ê¸°ë°˜ ì±—ë´‡ðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ. - ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ì½”ë“œë¡œ ìƒì„±í•œë‹¤.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# íƒ­ì„ ìƒì„±\\nmain_tab1, main_tab2 = st.tabs([\"ì´ë¯¸ì§€\", \"ëŒ€í™”ë‚´ìš©\"])\\n\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"ì´ë¯¸ì§€íŒŒì¼ ì—…ë¡œë“œ\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\"modelì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\"])\\n\\n    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€\\n    system_prompt = st.text_area(\\n        \"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\",\\n        \"ë‹¹ì‹ ì€ í‘œ(ìž¬ë¬´ì œí‘œ)ë¥¼ í•´ì„í•˜ëŠ” ê¸ˆìœµ AIì–´ì‹œìŠ¤í„´íŠ¸ ìž…ë‹ˆë‹¤. \\\\në‹¹ì‹ ì˜ ìž„ë¬´ëŠ” ì£¼ì–´ì§„ í…Œì´ë¸” í˜•ì‹ì˜ ìž¬ë¬´ì œí‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ í¥ë¯¸ë¡œìš´ ì‚¬ì‹¤ì„ ì •ë¦¬í•˜ì—¬ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ”ê²ƒ ìž…ë‹ˆë‹¤.\",\\n        height=200,\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¸Œë ¥ - chat_messageì—ëŠ” roleê³¼ contentê°€ ë“¤ì–´ê°€ ìžˆë‹¤.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        main_tab2.chat_message(chat_message.role).write(chat_message.content)\\n        # st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€ - ChatMessageì€ langchain_core.messages.chatì— ìžˆë‹¤.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì´ë¯¸ì§€ë¥¼ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\ndef process_imagefile(file):\\n    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # create_retrieverë¥¼ retrieverë¡œë¶€í„° ê°€ì ¸ì˜¨ë‹¤.\\n    return file_path\\n\\n\\n# ì´ë¯¸ì§€ì— ëŒ€í•œ ëŒ€ë‹µ ìƒì„±\\ndef generate_answer(\\n    image_filepath, system_prompt, user_prompt, model_name=\"gpt-4o-mini\"\\n):\\n    # model_nameìœ¼ë¡œ ë„£ì–´ì¤˜ì•¼ ë™ì ìœ¼ë¡œ ì‚¬ìš©ê°€ëŠ¥.\\n    llm = ChatOpenAI(temperature=0, model=model_name)\\n\\n    # ë©€í‹°ëª¨ëŒˆ ê°ì²´ ìƒì„±\\n    multimodal = MultiModal(llm, system_prompt=system_prompt, user_prompt=user_prompt)\\n\\n    # ì´ë¯¸ì§€ íŒŒì¼ë¡œ ë¶€í„° ì§ˆì˜(ìŠ¤íŠ¸ë¦¼ ë°©ì‹)\\n    answer = multimodal.stream(image_filepath)\\n    return answer\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ëœ¨ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = main_tab2.empty()\\n\\n# ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œê°€ ëœë‹¤ë©´\\nif uploaded_file:\\n    # ì´ë¯¸ì§€ íŒŒì¼ì„ ì²˜ë¦¬\\n    image_filepath = process_imagefile(uploaded_file)\\n    main_tab1.image(image_filepath)\\n    # st.image(image_filepath)\\n\\n# ë§Œì•½ ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´,\\nif user_input:\\n    # íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆëŠ”ì§€ í™•ì¸\\n    if uploaded_file:\\n        # process_imagefile í•¨ìˆ˜ë¥¼ í†µí•´ file uploaded : ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬\\n        image_filepath = process_imagefile(uploaded_file)\\n        # # ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œ í–ˆë‹¤ëŠ” ë©”ì„¸ì§€ ì¶œë ¥\\n        # warning_msg.success(\"ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\\n        # generate_answer í•¨ìˆ˜ì‚¬ìš© : ë‹µë³€ìš”ì²­\\n        response = generate_answer(\\n            image_filepath, system_prompt, user_input, selected_model\\n        )\\n\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        main_tab2.chat_message(\"user\").write(user_input)\\n\\n        with main_tab2.chat_message(\"assistant\"):\\n            # ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë°\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token.content\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n\\n    else:\\n        # ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì„¸ì§€\\n        warning_msg.error(\"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”\")\\n\\n# poetry shellë¡œ ê°€ìƒí™˜ê²½ ì‹¤í–‰\\n# streamlit run .\\\\02.Local_lag.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\04_Multi_turn.py'}, page_content='from urllib import response\\nfrom requests import session\\nimport streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_teddynote import logging\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\nfrom langchain_core.chat_history import BaseChatMessageHistory\\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n\\n# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ìž…ë ¥í•©ë‹ˆë‹¤.\\nlogging.langsmith(\"[Project] Multi Tuen ì±—ë´‡\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"ëŒ€í™”ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” ì±—ë´‡ ðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # ëª¨ë¸ ì„ íƒ ë©”ë‰´\\n    selected_model = st.selectbox(\"LLM ì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\"], index=0)\\n\\n    # ì„¸ì…˜ ID ë¥¼ ì§€ì •í•˜ëŠ” ë©”ë‰´\\n    session_id = st.text_input(\"ì„¸ì…˜ IDë¥¼ ìž…ë ¥í•˜ì„¸ìš”.\", \"abc123\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°\\n        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ìž¥\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(model_name=\"gpt-4o\"):\\n\\n    # í”„ë¡¬í”„íŠ¸ ì •ì˜\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ìž…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\",\\n            ),\\n            # ëŒ€í™”ê¸°ë¡ìš© key ì¸ chat_history ëŠ” ê°€ê¸‰ì  ë³€ê²½ ì—†ì´ ì‚¬ìš©í•˜ì„¸ìš”!\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\"human\", \"#Question:\\\\n{question}\"),  # ì‚¬ìš©ìž ìž…ë ¥ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©\\n        ]\\n    )\\n\\n    # llm ìƒì„±\\n    llm = ChatOpenAI(model_name=\"gpt-4o\")\\n\\n    # ì¼ë°˜ Chain ìƒì„±\\n    chain = prompt | llm | StrOutputParser()\\n\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\n        input_messages_key=\"question\",  # ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì´ í…œí”Œë¦¿ ë³€ìˆ˜ì— ë“¤ì–´ê°ˆ key\\n        history_messages_key=\"chat_history\",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤\\n    )\\n    return chain_with_history\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = create_chain(model_name=selected_model)\\n\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n    if chain is not None:\\n        response = chain.stream(\\n            # ì§ˆë¬¸ ìž…ë ¥\\n            {\"question\": user_input},\\n            # ì„¸ì…˜ ID ê¸°ì¤€ìœ¼ë¡œ ëŒ€í™”ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n            # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n            add_message(\"user\", user_input)\\n            add_message(\"assistant\", ai_answer)\\n    else:\\n        # ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥\\n        warning_msg.error(\"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\04_Multi_turn_practice copy 2.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.messages import ChatMessage  # ë©”ì„¸ì§€ ì¶”ê°€\\nfrom langchain_community.chat_message_histories import (\\n    ChatMessageHistory,\\n)  # ëŒ€í™”ê¸°ë¡ ì €ìž¥\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.prompts import (\\n    ChatPromptTemplate,\\n    MessagesPlaceholder,\\n)  # ëŒ€í™”ê¸°ë¡ ì €ìž¥\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.runnables.history import RunnableWithMessageHistory  # ëŒ€í™”ê¸°ë¡ ì €ìž¥\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project]Multi-turn\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„± .ì„ ì°ëŠ”ê±´ ìˆ¨ê¹€í‘œì‹œ ì˜ë¯¸\\nif not os.path.exists(\"./.cache\"):\\n    os.mkdir(\"./.cache\")\\n\\nif not os.path.exists(\"./.cache/files\"):\\n    os.mkdir(\"./.cache/files\")\\n\\nif not os.path.exists(\"./.cache/embeddings\"):\\n    os.mkdir(\"./.cache.embeddings\")\\n\\nst.title(\"ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” ì±—ë´‡\")\\n\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = {}\\n\\n# â˜…â˜…â˜… storeë¥¼ìºì‹±í•˜ëŠ” ì½”ë“œ. ì„¸ì…˜ ê¸°ë¡ì„ ì €ìž¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    uploaded_file = st.file_uploader(\"pdfíŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    selected_model = st.selectbox(\"ëª¨ë¸ì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\"], index=0)\\n\\n    system_prompt = st.text_area(\\n        \"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\", \"í”„ë¡¬í”„íŠ¸ë¥¼ ìž…ë ¥í•´ì£¼ì„¸ìš”.\", height=200\\n    )\\n    session_id = st.text_input(\"ì„¸ì…˜ì•„ì´ë””ë¥¼ ìž…ë ¥í•˜ì„¸ìš”\", \"abc123\")\\n\\n# from langchain_core.messages import ChatMessage\\n\\n\\n# ì´ì „ëŒ€í™” ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n@st.cache_resource(show_spinner=\"íŒŒì¼ì„ ì—…ë¡œë“œ ì¤‘ ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    ##### indexing\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# ì„¸ì…˜ ê¸°ë¡ì„ ì €ìž¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\\n# store = {}\\n# from langchain_community.chat_message_histories import ChatMessageHistory\\n\\n\\n# â˜…â˜…â˜…ì„¸ì…˜ idë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ storeë¥¼ st.session_state[\"store\"]ë¡œ ë°”ê¿”ì¤Œ\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]\\n\\n\\n# from langchain_core.runnables.history import RunnableWithMessageHistory\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n\\n    # prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ìž…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ í†µí•´ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.\",\\n            ),\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\"human\", \"#Question:\\\\n{question} #Context\\\\n{context}\"),\\n        ]\\n    )\\n\\n    llm = ChatOpenAI(model=model_name)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    # â˜…â˜…â˜…\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,\\n        input_messages_key=\"question\",\\n        history_messages_key=\"chat_history\",\\n    )\\n    return chain_with_history\\n\\n\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n\\nprint_messages()\\n\\n\\n# ìœ ì € ì§ˆë¬¸\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ìœ„í•¨\\nwarning_msg = st.empty()\\n\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = create_chain(retriever, model_name=selected_model)\\n\\nif user_input:\\n    # ì²´ì¸ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain:\\n        response = chain.stream(\\n            {\"question\": user_input},\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n\\n        st.chat_message(\"user\").write(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ ì£¼ì„¸ìš”\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\04_Multi_turn_practice.py'}, page_content='from urllib import response\\nfrom requests import session\\nimport os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF Multi-turn RAG\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDFê¸°ë°˜ ë©€í‹°í„´ QAðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ. - ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ì½”ë“œë¡œ ìƒì„±í•œë‹¤.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•ŠëŠ”ê²½ìš°\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"PDFíŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"modelì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n    # ì„¸ì…˜ ID ë¥¼ ì§€ì •í•˜ëŠ” ë©”ë‰´\\n    session_id = st.text_input(\"ì„¸ì…˜ IDë¥¼ ìž…ë ¥í•˜ì„¸ìš”.\", \"abc123\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¸Œë ¥ - chat_messageì—ëŠ” roleê³¼ contentê°€ ë“¤ì–´ê°€ ìžˆë‹¤.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€ - ChatMessageì€ langchain_core.messages.chatì— ìžˆë‹¤.\\ndef add_messages(role, message):\\n    # messageê°€ ë¬¸ìžì—´ì¸ì§€ í™•ì¸\\n    if isinstance(message, str):\\n        st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n    else:\\n        st.session_state[\"messages\"].append(\\n            ChatMessage(role=role, content=str(message))\\n        )\\n\\n\\n# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°\\n        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ìž¥\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------------------------ indexing\\n    # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í• \\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # 3ë‹¨ê³„: ìž„ë² ë”© ìƒì„±\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5ë‹¨ê³„: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±\\n    # prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ìž…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ í†µí•´ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.\",\\n            ),\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\"human\", \"#Question:\\\\n{question} #Context\\\\n{context}\"),\\n        ]\\n    )\\n\\n    # 7ë‹¨ê³„ ì–¸ì–´ëª¨ë¸ ìƒì„±\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8ë‹¨ê³„ ì²´ì¸ ìƒì„±\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\n        input_messages_key=\"question\",  # ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì´ í…œí”Œë¦¿ ë³€ìˆ˜ì— ë“¤ì–´ê°ˆ key\\n        history_messages_key=\"chat_history\",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤\\n    )\\n    return chain_with_history\\n\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ\\nif uploaded_file:\\n    # íŒŒì¼ ì—…ë¡œë“œ í›„ retrieverìƒì„±. ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦´ ì˜ˆì •\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\nelse:\\n    retriever = None\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\n\\nif st.session_state[\"chain\"] is None and retriever is not None:\\n    st.session_state[\"chain\"] = create_chain(retriever, model_name=selected_model)\\n\\n\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = create_chain(retriever, model_name=selected_model)\\n\\n# ë§Œì•½ ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´,\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n    if chain is not None:\\n        response = chain.stream(\\n            # ì§ˆë¬¸ ìž…ë ¥\\n            {\"question\": user_input},\\n            # ì„¸ì…˜ ID ê¸°ì¤€ìœ¼ë¡œ ëŒ€í™”ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token.content\\n                container.markdown(ai_answer)\\n\\n            # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n            add_messages(\"user\", user_input)\\n            add_messages(\"assistant\", ai_answer)\\n    else:\\n        # ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥\\n        warning_msg.error(\"pdfíŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\")\\n\\n\\n# poetry shellë¡œ ê°€ìƒí™˜ê²½ ì‹¤í–‰\\n# streamlit run PDF.py\\n\\n\\n# # ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\n# def get_session_history(session_ids):\\n#     if session_ids not in st.session_state[\"store\"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°\\n#         # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ìž¥\\n#         st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n#     return st.session_state[\"store\"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜\\n\\n\\n# # íŒŒì¼ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n# @st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\n# def embed_file(file):\\n#     # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥\\n#     file_content = file.read()\\n#     file_path = f\"./.cache/files/{file.name}\"\\n#     with open(file_path, \"wb\") as f:\\n#         f.write(file_content)\\n\\n#     # ------------------------------ indexing\\n#     # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ\\n#     loader = PDFPlumberLoader(file_path)\\n#     docs = loader.load()\\n\\n#     # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í• \\n#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n#     documents = text_splitter.split_documents(docs)\\n\\n#     # 3ë‹¨ê³„: ìž„ë² ë”© ìƒì„±\\n#     embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n#     # 4ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\\n#     vector_store = FAISS.from_documents(documents, embeddings)\\n\\n#     # 5ë‹¨ê³„: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±\\n#     retriever = vector_store.as_retriever()\\n#     return retriever\\n\\n\\n# # ì²´ì¸ ìƒì„±\\n# def create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n#     # 6ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±\\n#     # prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n#     prompt = ChatPromptTemplate.from_messages(\\n#         [\\n#             (\\n#                 \"system\",\\n#                 \"ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ìž…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ í†µí•´ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.\",\\n#             ),\\n#             MessagesPlaceholder(variable_name=\"chat_history\"),\\n#             (\"human\", \"#Question:\\\\n{question} #Context\\\\n{context}\"),\\n#         ]\\n#     )\\n\\n#     # 7ë‹¨ê³„ ì–¸ì–´ëª¨ë¸ ìƒì„±\\n#     llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n#     # 8ë‹¨ê³„ ì²´ì¸ ìƒì„±\\n#     chain = (\\n#         {\"context\": retriever, \"question\": RunnablePassthrough()}\\n#         | prompt\\n#         | llm\\n#         | StrOutputParser()\\n#     )\\n#     chain_with_history = RunnableWithMessageHistory(\\n#         chain,\\n#         get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\n#         input_messages_key=\"question\",  # ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì´ í…œí”Œë¦¿ ë³€ìˆ˜ì— ë“¤ì–´ê°ˆ key\\n#         history_messages_key=\"chat_history\",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤\\n#     )\\n#     return chain_with_history\\n\\n\\n# # íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ\\n# if uploaded_file:\\n#     # íŒŒì¼ ì—…ë¡œë“œ í›„ retrieverìƒì„±. ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦´ ì˜ˆì •\\n#     retriever = embed_file(uploaded_file)\\n#     chain = create_chain(retriever, model_name=selected_model)\\n#     st.session_state[\"chain\"] = chain\\n\\n# # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n# user_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = create_chain(retriever, model_name=selected_model)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\05_pdf-Multi_turn copy.py'}, page_content='from dotenv import load_dotenv\\nimport os\\nimport streamlit as st\\nfrom operator import itemgetter\\n\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n# kiwi\\nfrom kiwipiepy import Kiwi\\n\\nkiwi = Kiwi()\\n# Ensemble Retriever\\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\\n\\n# reranker\\nfrom langchain.retrievers import ContextualCompressionRetriever\\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker\\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder\\n\\n# reorder\\nfrom langchain_community.document_transformers import LongContextReorder\\nfrom langchain_core.runnables import RunnableLambda\\n\\nlogging.langsmith(\"[Project] PDF Multu-turn RAG\")\\n\\nload_dotenv()\\n\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\"cache\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\"cache/embeddings\")\\n\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\"cache/files\")\\n\\nst.title(\"PDFê¸°ë°˜ ë©€í‹°í„´ QA\")\\n\\n# ì²˜ìŒ í•œë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ - ë¦¬ìŠ¤íŠ¸ë¡œ ì €ìž¥í•˜ëŠ”ê²ƒì€ ìˆœì„œëŒ€ë¡œ ì €ìž¥í•˜ëŠ” íŠ¹ì§•ì´ ìžˆë‹¤\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# ë”•ì…”ë„ˆë¦¬ë¡œ ì €ìž¥í•˜ëŠ”ê²ƒì€ í‚¤ë²¨ë¥˜ ìŒìœ¼ë¡œ ì €ìž¥í•˜ëŠ” íŠ¹ì§•ì´ ìžˆë‹¤.\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"ëŒ€í™”ì´ˆê¸°í™”\")\\n\\n    uploaded_file = st.file_uploader(\"PDFíŒŒì¼ ì—…ë¡œë“œ\", type=[\"PDF\"])\\n\\n    selected_model = st.selectbox(\\n        \"modelì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n    # ì„¸ì…˜ IDë¥¼ ì§€ì •í•˜ëŠ” ë©”ë‰´\\n    session_id = st.text_input(\"ì„¸ì…˜ IDë¥¼ ìž…ë ¥í•´ ì£¼ì„¸ìš”\", \"abc123\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥ - chat_messageì—ëŠ” roleê³¼ contentê°€ ë“¤ì–´ê°€ ìžˆë‹¤.\\ndef print_message():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\nfrom langchain_core.messages import ChatMessage\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€ - ChatMessageì€ langchain_core.messages.chatì— ìžˆë‹¤.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\n\\n\\n# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]\\n\\n\\n# Kiwi í•¨ìˆ˜ : kiwi tokenizer\\ndef kiwi_tokenize(docs):\\n    return [token.form for token in kiwi.tokenize(docs)]\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # indexing\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n    # chroma\\n    chroma_vector_store = Chroma.from_documents(documents, embedding)\\n    chroma_retriever = chroma_vector_store.as_retriever()\\n    # kiwi + bm25\\n    kiwi_vector_store = BM25Retriever.from_documents(\\n        documents, preprocess_func=kiwi_tokenize\\n    )\\n    # ensemble\\n    ensemble_retriever = EnsembleRetriever(\\n        retrievers=[chroma_retriever, kiwi_vector_store],\\n        weights=[0.5, 0.5],\\n        search_kwargs={\"k\": 10},\\n    )\\n    # reranker\\n    reranker_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\\n    # ìƒìœ„ 3ê°œ ëª¨ë¸ ì„ íƒ\\n    compressor = CrossEncoderReranker(model=reranker_model, top_n=3)\\n    compression_retriever = ContextualCompressionRetriever(\\n        base_compressor=compressor, base_retriever=ensemble_retriever\\n    )\\n    return compression_retriever\\n\\n\\n# Reorder\\ndef reorder_documents(compression_retriever):\\n    # ìž¬ì •ë ¬\\n    reordering = LongContextReorder()\\n    reordered_docs = reordering.transform_documents(compression_retriever)\\n    return reordered_docs\\n\\n\\ndef create_chain(compression_retriever, model_name=\"gpt-4o-mini\"):\\n\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"\"\"\\n            ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ìž…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ í†µí•´ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”\\n            \"\"\",\\n            ),\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\\n                \"user\",\\n                \"\"\"\\n            #Question\\n            {question}\\n            #Context\\n            {context}\\n            \"\"\",\\n            ),\\n        ]\\n    )\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n    chain = (\\n        {\\n            \"context\": itemgetter(\"question\")\\n            | compression_retriever\\n            | reorder_documents,\\n            \"question\": itemgetter(\"question\"),\\n            \"chat_history\": itemgetter(\"chat_history\"),\\n        }\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    from langchain_core.runnables import RunnableWithMessageHistory\\n\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,\\n        input_messages_key=\"question\",\\n        history_messages_key=\"chat_history\",\\n    )\\n    return chain_with_history\\n\\n\\nif uploaded_file:\\n    compression_retriever = embed_file(uploaded_file)\\n    chain = create_chain(compression_retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_message()\\n\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”\")\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        response = chain.stream(\\n            {\"question\": user_input},\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ì–´ì„œ ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n            # ëŒ€í™”ê¸°ë¡ ì €ìž¥\\n            add_message(\"user\", user_input)\\n            add_message(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"pdf íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-ëª¨ë¸í™œìš© í”„ë¡œì íŠ¸â™¥\\\\pages\\\\05_pdf-Multi_turn.py'}, page_content='from operator import itemgetter\\nimport os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF Multi-turn RAG\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDFê¸°ë°˜ ë©€í‹°í„´ QAðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ. - ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ì½”ë“œë¡œ ìƒì„±í•œë‹¤.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•ŠëŠ”ê²½ìš°: storeì•ˆì— chainì´ ë“¤ì–´ê°€ ìžˆìœ¼ë‹ˆê¹Œ!\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = None\\n\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"PDFíŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"modelì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n    # ì„¸ì…˜ ID ë¥¼ ì§€ì •í•˜ëŠ” ë©”ë‰´\\n    session_id = st.text_input(\"ì„¸ì…˜ IDë¥¼ ìž…ë ¥í•˜ì„¸ìš”.\", \"abc123\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¸Œë ¥ - chat_messageì—ëŠ” roleê³¼ contentê°€ ë“¤ì–´ê°€ ìžˆë‹¤.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€ - ChatMessageì€ langchain_core.messages.chatì— ìžˆë‹¤.\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°\\n        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ìž¥\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------------------------ indexing\\n    # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í• \\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # 3ë‹¨ê³„: ìž„ë² ë”© ìƒì„±\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5ë‹¨ê³„: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±\\n    # prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ìž…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ í†µí•´ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.\",\\n            ),\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\"human\", \"#Question:\\\\n{question} #Context\\\\n{context}\"),\\n        ]\\n    )\\n\\n    # 7ë‹¨ê³„ ì–¸ì–´ëª¨ë¸ ìƒì„±\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8ë‹¨ê³„ ì²´ì¸ ìƒì„±\\n    chain = (\\n        {\\n            \"context\": itemgetter(\"question\") | retriever,  # ìˆ˜ì •\\n            \"question\": itemgetter(\"question\"),  # ìˆ˜ì •\\n            \"chat_history\": itemgetter(\"chat_history\"),  # ìˆ˜ì •c\\n        }\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\n        input_messages_key=\"question\",  # ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì´ í…œí”Œë¦¿ ë³€ìˆ˜ì— ë“¤ì–´ê°ˆ key\\n        history_messages_key=\"chat_history\",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤\\n    )\\n    return chain_with_history\\n\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ\\nif uploaded_file:\\n    # íŒŒì¼ ì—…ë¡œë“œ í›„ retrieverìƒì„±. ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦´ ì˜ˆì •\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\n# ìˆ˜ì •\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = create_chain(retriever, model_name=selected_model)\\n\\n# ë§Œì•½ ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´,\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n    if chain is not None:\\n        response = chain.stream(\\n            # ì§ˆë¬¸ ìž…ë ¥\\n            {\"question\": user_input},\\n            # ì„¸ì…˜ ID ê¸°ì¤€ìœ¼ë¡œ ëŒ€í™”ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n            # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n            add_messages(\"user\", user_input)\\n            add_messages(\"assistant\", ai_answer)\\n    else:\\n        # ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥\\n        warning_msg.error(\"pdfíŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\1.main.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\n\\nst.title(\"ë°©ì´ Chat GPT Test\")\\n\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ë‚´ìš©ì„ ì €ìž¥í•˜ëŠ” ê¸°ëŠ¥\\n    st.session_state[\"messages\"] = []\\n\\n\\n# ì´ì „ ëŒ€í™”ë‚´ìš©ì„ ì¶œë ¥, í”„ë¦°íŠ¸ í•¨ìˆ˜.\\n# for role, message in st.session_state[\"messages\"]:\\n#     st.chat_message(role).write(message)\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n        # st.write(f\"{chat_message.role}: {chat_message.comtent}\")\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¬.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\nprint_messages()\\n\\n\\n# ì‚¬ìš©ìž ìž…ë ¥ì°½\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”\")\\n\\n# ë§Œì•½ ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´..\\n# ì €ìž¥, messageì˜ ê²½ìš° contrainerì•ˆì— ë‹´ì•„ì£¼ëŠ” ì—­í• .\\nif user_input:\\n    # st.write(f\"ì‚¬ìš©ìž ìž…ë ¥: {user_input}\")\\n    st.chat_message(\"user\").write(user_input)  # ìž…ë ¥\\n    st.chat_message(\"assistant\").write(user_input)  # aië„ ê·¸ëŒ€ë¡œ ìž…ë ¥\\n\\n    # ëŒ€í™”ë‚´ìš©ì„ ì €ìž¥. ìœ„ì˜ st.session_state[\"messages\"] = []ë¥¼ ë°›ëŠ”ë‹¤.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", user_input)\\n    # ChatMessage(role=\"user\", content=user_input)\\n    # ChatMessage(role=\"assistant\", content=user_input)\\n    # st.session_state[\"messages\"].append((\"user\", user_input))\\n    # st.session_state[\"messages\"].append((\"assistant\", user_input))\\n\\n## Terminalì°½ì—ì„œ streamlitì„ ì¼œì¤€ë‹¤.\\n# streamlit run .\\\\19-Streamlit\\\\MyProject\\\\main.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\2.main.py'}, page_content='import streamlit as st\\n\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom dotenv import load_dotenv\\n\\n# API_KEY ì •ë³´ ë¡œë“œ\\nload_dotenv()\\n\\nst.title(\"ë°©ì´ Chat GPT Test\")\\n\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ë‚´ìš©ì„ ì €ìž¥í•˜ëŠ” ê¸°ëŠ¥\\n    st.session_state[\"messages\"] = []\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™”ì´ˆê¸°í™”\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë‚´ìš©ì„ ì¶œë ¥, í”„ë¦°íŠ¸ í•¨ìˆ˜.\\n# for role, message in st.session_state[\"messages\"]:\\n#     st.chat_message(role).write(message)\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n        # st.write(f\"{chat_message.role}: {chat_message.comtent}\")\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¬.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì²´ì¸ìƒì„±\\ndef create_chain():\\n    # prompt | llm | output_parser\\n    # prompt\\n    prompt = ChatPromptTemplate.from_messages(\\n        {\\n            (\"system\", \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ìž…ë‹ˆë‹¤.\"),  # ì „ì—­ë³€ìˆ˜ë¡œ, ì§€ì‹œì‚¬í•­\\n            (\"user\", \"#Question:\\\\n{question}\"),  # ìž…ë ¥\\n        }\\n    )\\n    # model\\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\n\\n    # ì¶œë ¥ íŒŒì„œ\\n    output_parser = StrOutputParser()\\n\\n    # ì²´ì¸ ìƒì„±\\n    chain = prompt | llm | output_parser\\n\\n    return chain\\n\\n\\n### ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´, ë¹ˆ listë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤.\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n### ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n\\n# ì‚¬ìš©ìž ìž…ë ¥ì°½\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”\")\\n\\n# ë§Œì•½ ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´..\\n# ì €ìž¥, messageì˜ ê²½ìš° contrainerì•ˆì— ë‹´ì•„ì£¼ëŠ” ì—­í• .\\nif user_input:\\n    # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n    st.chat_message(\"user\").write(user_input)  # ìž…ë ¥\\n    # chainì„ ìƒì„±\\n    chain = create_chain()\\n    # ai_answer = chain.invoke({\"question\": user_input})\\n\\n    ## í•œê¸€ìžì”© ì¶œë ¥í•˜ê¸° ìœ„í•œ streamì´ìš©\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):  # ì§ˆë¬¸ì„\\n        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:  # tokenì„ í•˜ë‚˜ì”© ë°›ëŠ”ë‹¤.\\n            ai_answer += token  # answerì—ë‹¤ê°€ tokenì„ ëˆ„ì ì‹œí‚¨ë‹¤.\\n            container.markdown(ai_answer)\\n\\n    # ai_ì˜ ë‹µë³€\\n    # st.chat_message(\"assistant\").write(ai_answer)\\n    # st.chat_message(\"assistant\").write(user_input)  # aië„ ê·¸ëŒ€ë¡œ ìž…ë ¥\\n\\n    # ëŒ€í™”ë‚´ìš©ì„ ì €ìž¥. ìœ„ì˜ st.session_state[\"messages\"] = []ë¥¼ ë°›ëŠ”ë‹¤.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n    # add_message(\"assistant\", user_input)\\n\\n## Terminalì°½ì—ì„œ streamlitì„ ì¼œì¤€ë‹¤.\\n# streamlit run .\\\\19-Streamlit\\\\MyProject\\\\main.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\3.main.py'}, page_content='import streamlit as st\\n\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain import hub\\n\\nfrom dotenv import load_dotenv\\n\\n# API_KEY ì •ë³´ ë¡œë“œ\\nload_dotenv()\\n\\nst.title(\"ë°©ì´ Chat GPT Test\")\\n\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ë‚´ìš©ì„ ì €ìž¥í•˜ëŠ” ê¸°ëŠ¥\\n    st.session_state[\"messages\"] = []\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # ì…€ë ‰íŠ¸ ë°•ìŠ¤ ë§Œë“¤ê¸°. ê¸°ë³¸ëª¨ë“œ , SNS ê²Œì‹œê¸€, ìš”ì•½ 3ê°œê°€ ì¶œë ¥ëŒ.\\n    selected_prompt = st.selectbox(\\n        \"í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”\",\\n        (\"ê¸°ë³¸ëª¨ë“œ\", \"SNS ê²Œì‹œê¸€\", \"ìš”ì•½\"),\\n        index=0,\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë‚´ìš©ì„ ì¶œë ¥, í”„ë¦°íŠ¸ í•¨ìˆ˜.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n        # ì—­í• ê³¼, ë‚´ìš©ë§Œ ì¶œë ¥\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¬.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì²´ì¸ìƒì„± - ë§¤ê°œë³€ìˆ˜ì— promptì¶”ê°€\\ndef create_chain(prompt_type):\\n    # prompt | llm | output_parser\\n    # prompt (ê¸°ë³¸ëª¨ë“œ)\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ìž…ë‹ˆë‹¤. ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹´ë³€í•´ ì£¼ì„¸ìš”.\",\\n            ),  # ì „ì—­ë³€ìˆ˜ë¡œ, ì§€ì‹œì‚¬í•­\\n            (\"user\", \"#Question:\\\\n{question}\"),  # ìž…ë ¥\\n        ]\\n    )\\n\\n    # SNS ë§ˆì¼€í„° í”„ë¡¬í”„íŠ¸\\n    if prompt_type == \"SNS ê²Œì‹œê¸€\":\\n        prompt = load_prompt(\\n            \"19-Streamlit/MyProject/prompts/sns.yaml\", encoding=\"utf-8\"\\n        )\\n        # load_prompt(\"prompts/sns.yaml\", encoding=\"utf-8\")\\n\\n    # 19-Streamlit\\\\MyProject\\\\prompts\\\\sns.yaml\\n\\n    # ìš”ì•½ prompt\\n    elif prompt_type == \"ìš”ì•½\":\\n        prompt = hub.pull(\"teddynote/chain-of-density-korean:946ed62d\")\\n\\n    # model\\n    # llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\n\\n    # ì¶œë ¥ íŒŒì„œ\\n    output_parser = StrOutputParser()\\n\\n    # ì²´ì¸ ìƒì„±\\n    chain = prompt | llm | output_parser\\n\\n    return chain\\n\\n\\n### ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´, ë¹ˆ listë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤.\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n### ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n\\n# ì‚¬ìš©ìž ìž…ë ¥ì°½\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ë§Œì•½ ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´..\\n# ì €ìž¥, messageì˜ ê²½ìš° contrainerì•ˆì— ë‹´ì•„ì£¼ëŠ” ì—­í• .\\nif user_input:\\n    # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n    st.chat_message(\"user\").write(user_input)  # ìž…ë ¥\\n\\n    # chainì„ ìƒì„±\\n    chain = create_chain(selected_prompt)\\n    # ai_answer = chain.invoke({\"question\": user_input})\\n\\n    ## ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ:í•œê¸€ìžì”© ì¶œë ¥í•˜ê¸° ìœ„í•œ streamì´ìš©\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):  # ì§ˆë¬¸ì„\\n        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:  # tokenì„ í•˜ë‚˜ì”© ë°›ëŠ”ë‹¤.\\n            ai_answer += token  # answerì—ë‹¤ê°€ tokenì„ ëˆ„ì ì‹œí‚¨ë‹¤.\\n            container.markdown(ai_answer)  # ai_answerë¥¼ markdowní˜•ì‹ìœ¼ë¡œ ì €ìž¥.\\n\\n    # ëŒ€í™”ë‚´ìš©ì„ ì €ìž¥. ìœ„ì˜ st.session_state[\"messages\"] = []ë¥¼ ë°›ëŠ”ë‹¤.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n    # add_message(\"assistant\", user_input)\\n\\n## Terminalì°½ì—ì„œ streamlitì„ ì¼œì¤€ë‹¤.\\n# streamlit run 19-Streamlit\\\\MyProject\\\\main.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\4.main.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom dotenv import load_dotenv\\nfrom langchain import hub\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n\\nst.title(\"ë‚˜ë§Œì˜ ì±—GPTðŸ’¬\")\\n\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    selected_prompt = st.selectbox(\\n        \"í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”\",\\n        (\"ê¸°ë³¸ëª¨ë“œ\", \"SNS ê²Œì‹œê¸€\", \"ìš”ì•½\"),\\n        index=0,\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(prompt_type):\\n    # prompt | llm | output_parser\\n    # í”„ë¡¬í”„íŠ¸(ê¸°ë³¸ëª¨ë“œ)\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\",\\n            ),\\n            (\"user\", \"#Question:\\\\n{question}\"),\\n        ]\\n    )\\n    if prompt_type == \"SNS ê²Œì‹œê¸€\":\\n        # Windows ì‚¬ìš©ìž only: ì¸ì½”ë”©ì„ cp949ë¡œ ì„¤ì •\\n        prompt = load_prompt(\\n            \"19-Streamlit/MyProject/prompts/sns.yaml\", encoding=\"utf-8\"\\n        )\\n    elif prompt_type == \"ìš”ì•½\":\\n        # ìš”ì•½ í”„ë¡¬í”„íŠ¸\\n        prompt = hub.pull(\"teddynote/chain-of-density-korean:946ed62d\")\\n\\n    # GPT\\n    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\\n\\n    # ì¶œë ¥ íŒŒì„œ\\n    output_parser = StrOutputParser()\\n\\n    # ì²´ì¸ ìƒì„±\\n    chain = prompt | llm | output_parser\\n    return chain\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n    st.chat_message(\"user\").write(user_input)\\n    # chain ì„ ìƒì„±\\n    chain = create_chain(selected_prompt)\\n\\n    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):\\n        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\5.main.py'}, page_content='import streamlit as st\\nimport glob\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom dotenv import load_dotenv\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n\\nst.title(\"ë‚˜ë§Œì˜ ì±—GPTðŸ’¬\")\\n\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # import glob\\n\\n    # íŠ¹ì • ë¬¸ìžì—´ ê·œì¹™ì„ ì£¼ë©´, ê·¸ ê·œì¹™ ì•ˆì—ìžˆëŠ” íŠ¹ì • íŒŒì¼ì„ ì­‰ ê°€ì ¸ì™€ì„œ listë¡œ ì €ìž¥í•´ì¤€ë””.\\n    prompt_files = glob.glob(\"prompts/*.yaml\")\\n    selected_prompt = st.selectbox(\"í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”\", prompt_files, index=0)\\n    task_input = st.text_input(\"TASKìž…ë ¥\", \"\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# def print_messages():\\n#     for chat_message in st.session_state[\"messages\"]:\\n#         st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# def add_message(role, message):\\n#     st.session_state[\"mesasges\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(prompt_filepath, task=\"\"):\\n\\n    # prompt | llm | output_parser\\n    prompt = load_prompt(prompt_filepath, encoding=\"utf-8\")\\n\\n    if task:\\n        prompt = prompt.partial(task=task)\\n\\n    # GPT\\n    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\\n\\n    # ì¶œë ¥ íŒŒì„œ\\n    output_parser = StrOutputParser()\\n\\n    # ì²´ì¸ ìƒì„±\\n    chain = prompt | llm | output_parser\\n    return chain\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n    st.chat_message(\"user\").write(user_input)\\n    # chain ì„ ìƒì„±\\n    chain = create_chain(selected_prompt, task=task_input)\\n\\n    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):\\n        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\6.main.py'}, page_content='import streamlit as st\\nimport glob\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom dotenv import load_dotenv\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n\\nst.title(\"ë‚˜ë§Œì˜ ì±—GPTðŸ’¬\")\\n\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # import glob\\n\\n    # íŠ¹ì • ë¬¸ìžì—´ ê·œì¹™ì„ ì£¼ë©´, ê·¸ ê·œì¹™ ì•ˆì—ìžˆëŠ” íŠ¹ì • íŒŒì¼ì„ ì­‰ ê°€ì ¸ì™€ì„œ listë¡œ ì €ìž¥í•´ì¤€ë””.\\n    prompt_files = glob.glob(\"prompts/*.yaml\")\\n    selected_prompt = st.selectbox(\"í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”\", prompt_files, index=0)\\n    task_input = st.text_input(\"TASKìž…ë ¥\", \"\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# def print_messages():\\n#     for chat_message in st.session_state[\"messages\"]:\\n#         st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# def add_message(role, message):\\n#     st.session_state[\"mesasges\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(prompt_filepath, task=\"\"):\\n\\n    # prompt | llm | output_parser\\n    prompt = load_prompt(prompt_filepath, encoding=\"utf-8\")\\n\\n    if task:\\n        prompt = prompt.partial(task=task)\\n\\n    # GPT\\n    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\\n\\n    # ì¶œë ¥ íŒŒì„œ\\n    output_parser = StrOutputParser()\\n\\n    # ì²´ì¸ ìƒì„±\\n    chain = prompt | llm | output_parser\\n    return chain\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n    st.chat_message(\"user\").write(user_input)\\n    # chain ì„ ìƒì„±\\n    chain = create_chain(selected_prompt, task=task_input)\\n\\n    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):\\n        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\pages\\\\01_PDF.py'}, page_content='import streamlit as st\\nimport os\\nfrom dotenv import load_dotenv\\n\\nfrom langchain_teddynote import logging\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n# api key ê°€ì ¸ì˜¤ê¸°\\nload_dotenv()\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„± .ì€ ìˆ¨ê¹€íŒŒì¼ë¡œ ì²˜ë¦¬í•œë‹¤.\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\n\\nst.title(\"PDFê¸°ë°˜ QAðŸ’¬\")\\n\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"ëŒ€í™”ì´ˆê¸°í™”\")\\n\\n    uploaded_file = st.file_uploader(\"íŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # selected_prompt = \"prompts/pdf-rag.yaml\"\\n\\n    selected_model = st.selectbox(\\n        \"modelì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥.(ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n    ############ indexing\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    output_parser = StrOutputParser()\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | output_parser\\n    )\\n    return chain\\n\\n\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nif uploaded_file:\\n    # íŒŒì¼ ì—…ë¡œë“œ í›„ retrieverìƒì„±. ìž‘ì—…ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦´ ì˜ˆì •\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\nprint_messages()\\n\\nuser_input = st.chat_input(\"ë©”ì„¸ì§€ë¥¼ ìž…ë ¥í•´ ì£¼ì„¸ìš”\")\\n\\n# ë¹ˆ ì˜ì—­ì„ ìž¡ì•„ì£¼ëŠ” ì—­í• . ê²½ê³  ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•¨\\nwarning_msg = st.empty()\\n\\nif user_input:\\n\\n    # ì²´ì¸ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™”ê¸°ë¡ ì €ìž¥\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n\\n    # íŒŒì¼ì„ ì—…ë¡œë“œí•˜ëŠ” ê²½ê³ ë©”ì„¸ì§€\\n    else:\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ ì£¼ì„¸ìš”\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\practice\\\\1.py'}, page_content='import streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\n\\nload_dotenv()\\n\\nst.title(\"ëŽ¨ë°©ì˜ GPT\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n\\ndef print_history():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_message(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\ndef print_history():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_message(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, conmtent=content))\\n\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n\\ndef print_history():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_message(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\ndef create_chain(prompt, model):\\n    model = ChatOpenAI(model=model)\\n\\n    chain = prompt | model | StrOutputParser()\\n    return chain\\n\\n\\nwith st.sidebar:\\n    clear_btn = st.buttion(\"ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”\")\\n    tab1, tab2 = st.tabs([\"í”„ë¡¬í”„íŠ¸\", \"í”„ë¦¬ì…‹\"])\\n    prompt = \"\"\"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ê°„ë‹¨ížˆ ë‹µí•´ì£¼ì„¸ìš”.\"\"\"\\n    user_text_prompt = tab1.text_area(\"í”„ë¡¬í”„íŠ¸\", value=prompt)\\n    user_text_apply_btn = tab1.button(\"í”„ë¡¬í”„íŠ¸ ì ìš©\", key=\"apply1\")\\n\\n    if user_text_apply_btn:\\n        tab1.markdown(f\"ë¥´í¼ë¥´íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.\")\\n        prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n        prompt = PromptTemplate.from_template(prompt_template)\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-4o-mini\")\\n\\n    user_selected_prompt = tab2.selectbox(\"í”„ë¦¬ì…‹ ì„ íƒ\", [\"sns\", \"ë²ˆì—­\", \"ìš”ì•½\"])\\n    user_selected_apply_btn = tab2.button(\"í”„ë¡¬í”„íŠ¸ ì ìš©\", key=\"apply2\")\\n    if user_selected_apply_btn:\\n        tab2.markdown(\"í”„ë¡¬í”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.\")\\n        prompt = load_prompt(f\"prompts/{user_selected_prompt}.yaml\", encoding=\"utf-8\")\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-4o-mini\")\\n\\nif clear_btn:\\n    retriever = st.session_state[\"messages\"].clear()\\n\\nprint_history\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\practice\\\\pdf_rag.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_teddynote.prompts import load_prompt\\n\\nload_dotenv()\\n\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\"cache\"):\\n    os.mkdir(\"cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\"cache/files\"):\\n    os.mkdir(\"cache/files\")\\n\\nif not os.path.exists(\"cache/embeddings\"):\\n    os.mkdir(\"cache/embeddings\")\\n\\n#########\\nst.title(\"PDFê¸°ë°˜ QA\")\\n\\n# ëŒ€í™”ê¸°ë¡ ì €ìž¥ì„ ìœ„í•œ session_state ìƒì„±\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•Šì€ ê²½ìš°\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\n# ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ ì¶”ê°€\\ndef add_messages(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"íŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"ëª¨ë¸ ì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥ + indexing\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ----------- Indexing\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# ì²´ì¸ìƒì„±\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ,\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”\")\\n\\n# ê²½ê³ ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\n# ë§Œì•½ ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´,\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(\"assistant\")\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ì–´, ì´ ì»¨í…Œì´ë„ˆì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë°\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™” ê¸°ë¡ì„ ì €ìž¥\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ ì£¼ì„¸ìš”\")\\n\\n# if \"messages\" not in st.session_state:\\n#     st.session_state[\"messages\"] = []\\n\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = None\\n\\n# def print_messages():\\n#     for chat_message in st.session_state[\"messages\"]:\\n#         st.chat_message(chat_message.role).write(chat_message.content)\\n\\n# def add_messages(role, content):\\n#     st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n# @st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\n# def embed_file(file):\\n#     file_content = file.read()\\n#     file_path = f\"cache/files/{file.name}\"\\n#     with open(file_path, \"wb\") as f:\\n#         f.write(file_content)\\n# \\'wb is write binary, rb is read binary, ab is add binary\\n\\n\\n# if \"messages\" not in st.session_state:\\n#     st.session_state[\"messages\"] = []\\n\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = None\\n\\n\\n# def print_messages():\\n#     for chat_message in st.session_state[\"messages\"]:\\n#         st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# def add_messages(role, content):\\n#     st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\n# @st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ìž…ë‹ˆë‹¤.\")\\n# def embed_file(file):\\n#     file_content = file.read()\\n#     file_path = f\"cache/files/{file.name}\"\\n#     with open(file_path, \"wb\") as f:\\n#         f.write(file_content)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\practice\\\\pdf_rag2.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nload_dotenv()\\n\\nif not os.path.exists(\"cache\"):\\n    os.mkdir(\"cache\")\\n\\nif not os.path.exists(\"cache/files\"):\\n    os.mkdir(\"cache/files\")\\n\\nif not os.path.exists(\"cache/embeddings\"):\\n    os.mkdir(\"cache/embeddings\")\\n\\nst.title(\"pdfê¸°ë°˜ qa\")\\n\\n# ëŒ€í™”ê¸°ë¡ ì €ìž¥ì„ ìœ„í•œ session_state\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_messages(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\nwith st.sidebar:\\n    # ì´ˆê¸°í™”ë²„íŠ¼\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"íŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"ëª¨ë¸ì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ë° ì¸ë±ì‹±\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    loader = PyPDFLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n\\n    return chain\\n\\n\\n#########################################\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ë©´\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    # chain ê°ì²´ë¥¼ st.session_state[\"chain\"]ì— ì €ìž¥\\n    st.session_state[\"chain\"] = chain\\n\\n# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ì„ ëˆ„ë¥´ë©´\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nprint_messages()\\n\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³  ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    # ì²´ì¸ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\practice\\\\pdf_rag3.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nload_dotenv()\\n\\nif not os.path.exists(\"cache\"):\\n    os.mkdir(\"cache\")\\n\\nif not os.path.exists(\"cache/files\"):\\n    os.mkdir(\"cache/files\")\\n\\nif not os.path.exists(\"cache/embeddings\"):\\n    os.mkdir(\"cache/embeddings\")\\n\\nst.title(\"pdfê¸°ë°˜ qa\")\\n\\n# ëŒ€í™”ê¸°ë¡ ì €ìž¥ì„ ìœ„í•œ session_state\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_messages(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\nwith st.sidebar:\\n    # ì´ˆê¸°í™”ë²„íŠ¼\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"íŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ë©”ë‰´ ì„ íƒ\\n    selected_model = st.selectbox(\\n        \"ëª¨ë¸ì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ë° ì¸ë±ì‹±\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    loader = PyPDFLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n\\n    return chain\\n\\n\\n#########################################\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ë©´\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    # chain ê°ì²´ë¥¼ st.session_state[\"chain\"]ì— ì €ìž¥\\n    st.session_state[\"chain\"] = chain\\n\\n# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ì„ ëˆ„ë¥´ë©´\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nprint_messages()\\n\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.\")\\n\\n# ê²½ê³  ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    # ì²´ì¸ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”\")\\n'),\n",
       " Document(metadata={'source': '..\\\\31.Agent-CrewAI\\\\coding\\\\extract_keywords.py'}, page_content='# filename: extract_keywords.py\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n# NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ì²˜ìŒ í•œ ë²ˆë§Œ ì‹¤í–‰)\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'stopwords\\')\\n\\n# ì˜ˆì‹œ ë¬¸ìž¥\\nsentence = \"Artificial intelligence is transforming the way we live and work.\"\\n\\n# ë‹¨ì–´ í† í°í™”\\nwords = word_tokenize(sentence)\\n\\n# ë¶ˆìš©ì–´ ì œê±°\\nstop_words = set(stopwords.words(\\'english\\'))\\nkeywords = [word for word in words if word.isalnum() and word.lower() not in stop_words]\\n\\nprint(keywords)'),\n",
       " Document(metadata={'source': '..\\\\31.Agent-CrewAI\\\\coding\\\\find_primes.py'}, page_content='# filename: find_primes.py\\ndef is_prime(n):\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\nprimes = [num for num in range(1, 101) if is_prime(num)]\\nprint(primes)'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\coding\\\\extract_keywords.py'}, page_content='# filename: extract_keywords.py\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n# NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'stopwords\\')\\n\\nsentence = \"The quick brown fox jumps over the lazy dog.\"\\ntokens = word_tokenize(sentence)\\nstop_words = set(stopwords.words(\\'english\\'))\\n\\nkeywords = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\\nprint(keywords)'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\coding\\\\extract_keywords_new.py'}, page_content='# filename: extract_keywords_new.py\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n# NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ì£¼ì„ ì²˜ë¦¬)\\n# nltk.download(\\'punkt\\')\\n# nltk.download(\\'stopwords\\')\\n\\nsentence = \"Artificial intelligence is transforming the way we live and work.\"\\ntokens = word_tokenize(sentence)\\nstop_words = set(stopwords.words(\\'english\\'))\\n\\nkeywords = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\\nprint(keywords)'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\coding\\\\find_primes.py'}, page_content='# filename: find_primes.py\\ndef is_prime(n):\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\nprimes = [num for num in range(1, 101) if is_prime(num)]\\nprint(primes)'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\coding\\\\install_and_extract_keywords.py'}, page_content='# filename: install_and_extract_keywords.py\\nimport subprocess\\nimport sys\\n\\n# NLTK ì„¤ì¹˜\\nsubprocess.check_call([sys.executable, \\'-m\\', \\'pip\\', \\'install\\', \\'nltk\\'])\\n\\nimport nltk\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'stopwords\\')\\n\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\nsentence = \"The quick brown fox jumps over the lazy dog.\"\\ntokens = word_tokenize(sentence)\\nstop_words = set(stopwords.words(\\'english\\'))\\n\\nkeywords = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\\nprint(keywords)'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\download_and_check.py'}, page_content='# filename: download_and_check.py\\nimport pandas as pd\\n\\n# ë°ì´í„° ë‹¤ìš´ë¡œë“œ\\nurl = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\\ndata = pd.read_csv(url)\\n\\n# ë°ì´í„°ì…‹ì˜ ì—´ ì¶œë ¥\\nprint(data.columns.tolist())'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\download_and_plot.py'}, page_content='# filename: download_and_plot.py\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# ë°ì´í„° ë‹¤ìš´ë¡œë“œ\\nurl = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\\ndata = pd.read_csv(url)\\n\\n# ë°ì´í„°ì…‹ì˜ ì—´ ì¶œë ¥\\nprint(data.columns.tolist())\\n\\n# \\'age\\'ì™€ \\'pclass\\' ê°„ì˜ ê´€ê³„ ì‹œê°í™”\\nplt.figure(figsize=(10, 6))\\nplt.scatter(data[\\'pclass\\'], data[\\'age\\'], alpha=0.5)\\nplt.title(\\'Relationship between Age and Passenger Class\\')\\nplt.xlabel(\\'Passenger Class (pclass)\\')\\nplt.ylabel(\\'Age\\')\\nplt.grid(True)\\n\\n# ì°¨íŠ¸ íŒŒì¼ë¡œ ì €ìž¥\\nplt.savefig(\\'age_pclass_relationship.png\\')\\nplt.close()'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\download_and_plot_with_check.py'}, page_content='# filename: download_and_plot_with_check.py\\ntry:\\n    import pandas as pd\\n    import matplotlib.pyplot as plt\\nexcept ImportError as e:\\n    print(\"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì¹˜í•˜ì„¸ìš”:\")\\n    print(\"pip install pandas matplotlib\")\\n    raise e\\n\\n# ë°ì´í„° ë‹¤ìš´ë¡œë“œ\\nurl = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\\ndata = pd.read_csv(url)\\n\\n# ë°ì´í„°ì…‹ì˜ ì—´ ì¶œë ¥\\nprint(data.columns.tolist())\\n\\n# \\'age\\'ì™€ \\'pclass\\' ê°„ì˜ ê´€ê³„ ì‹œê°í™”\\nplt.figure(figsize=(10, 6))\\nplt.scatter(data[\\'pclass\\'], data[\\'age\\'], alpha=0.5)\\nplt.title(\\'Relationship between Age and Passenger Class\\')\\nplt.xlabel(\\'Passenger Class (pclass)\\')\\nplt.ylabel(\\'Age\\')\\nplt.grid(True)\\n\\n# ì°¨íŠ¸ íŒŒì¼ë¡œ ì €ìž¥\\nplt.savefig(\\'age_pclass_relationship.png\\')\\nplt.close()'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\download_and_plot_with_enhanced_check.py'}, page_content='# filename: download_and_plot_with_enhanced_check.py\\nimport subprocess\\nimport sys\\n\\n# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\\ntry:\\n    import pandas as pd\\n    import matplotlib.pyplot as plt\\nexcept ImportError:\\n    print(\"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì¹˜í•˜ì„¸ìš”:\")\\n    print(\"pip install pandas matplotlib\")\\n    sys.exit(1)\\n\\n# ë°ì´í„° ë‹¤ìš´ë¡œë“œ\\nurl = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\\ndata = pd.read_csv(url)\\n\\n# ë°ì´í„°ì…‹ì˜ ì—´ ì¶œë ¥\\nprint(data.columns.tolist())\\n\\n# \\'age\\'ì™€ \\'pclass\\' ê°„ì˜ ê´€ê³„ ì‹œê°í™”\\nplt.figure(figsize=(10, 6))\\nplt.scatter(data[\\'pclass\\'], data[\\'age\\'], alpha=0.5)\\nplt.title(\\'Relationship between Age and Passenger Class\\')\\nplt.xlabel(\\'Passenger Class (pclass)\\')\\nplt.ylabel(\\'Age\\')\\nplt.grid(True)\\n\\n# ì°¨íŠ¸ íŒŒì¼ë¡œ ì €ìž¥\\nplt.savefig(\\'age_pclass_relationship.png\\')\\nplt.close()\\n\\nprint(\"ì°¨íŠ¸ê°€ \\'age_pclass_relationship.png\\'ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\download_and_plot_with_final_check.py'}, page_content='# filename: download_and_plot_with_final_check.py\\nimport subprocess\\nimport sys\\n\\ntry:\\n    import pandas as pd\\n    import matplotlib.pyplot as plt\\nexcept ImportError:\\n    print(\"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì¹˜í•˜ì„¸ìš”:\")\\n    print(\"pip install pandas matplotlib\")\\n    sys.exit(1)\\n\\n# ë°ì´í„° ë‹¤ìš´ë¡œë“œ\\nurl = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\\ndata = pd.read_csv(url)\\n\\n# ë°ì´í„°ì…‹ì˜ ì—´ ì¶œë ¥\\nprint(data.columns.tolist())\\n\\n# \\'age\\'ì™€ \\'pclass\\' ê°„ì˜ ê´€ê³„ ì‹œê°í™”\\nplt.figure(figsize=(10, 6))\\nplt.scatter(data[\\'pclass\\'], data[\\'age\\'], alpha=0.5)\\nplt.title(\\'Relationship between Age and Passenger Class\\')\\nplt.xlabel(\\'Passenger Class (pclass)\\')\\nplt.ylabel(\\'Age\\')\\nplt.grid(True)\\n\\n# ì°¨íŠ¸ íŒŒì¼ë¡œ ì €ìž¥\\nplt.savefig(\\'age_pclass_relationship.png\\')\\nplt.close()\\n\\nprint(\"ì°¨íŠ¸ê°€ \\'age_pclass_relationship.png\\'ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\tmp_code_eb0c39b18ee354c7f3b81f78817036ce.py'}, page_content='     import pandas as pd\\n     import matplotlib.pyplot as plt\\n     print(pd.__version__)\\n     print(plt.__version__)'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\07-DocumentLoader\\\\data\\\\audio_utils.py'}, page_content='import re\\nimport os\\nfrom pytube import YouTube\\nfrom moviepy.editor import AudioFileClip, VideoFileClip\\nfrom pydub import AudioSegment\\nfrom pydub.silence import detect_nonsilent\\n\\n\\ndef extract_abr(abr):\\n    youtube_audio_pattern = re.compile(r\"\\\\d+\")\\n    kbps = youtube_audio_pattern.search(abr)\\n    if kbps:\\n        kbps = kbps.group()\\n        return int(kbps)\\n    else:\\n        return 0\\n\\n\\ndef get_audio_filepath(filename):\\n    # audio í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\\n    if not os.path.isdir(\"audio\"):\\n        os.mkdir(\"audio\")\\n\\n    # í˜„ìž¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ì ˆëŒ€ ê²½ë¡œ ì–»ê¸°\\n    current_directory = os.path.abspath(\"\")\\n\\n    # íŒŒì¼ ê²½ë¡œ ìƒì„±\\n    audio_file_path = os.path.join(current_directory, \"audio\", filename)\\n\\n    return audio_file_path\\n\\n\\ndef convert_mp4_to_wav(mp4_file_path, wav_file_path):\\n    # MP4 íŒŒì¼ ë¡œë“œ\\n    audio_clip = AudioFileClip(mp4_file_path)\\n\\n    # WAV í˜•ì‹ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° ì €ìž¥\\n    audio_clip.write_audiofile(wav_file_path, fps=44100, nbytes=2, codec=\"pcm_s16le\")\\n\\n\\ndef download_audio_from_youtube(link):\\n    # YouTube ê°ì²´ ìƒì„±\\n    yt = YouTube(link)\\n\\n    # mp4 ì˜¤ë””ì˜¤ë§Œ í•„í„°ë§\\n    mp4_files = dict()\\n\\n    # \"audio/mp4\" íƒ€ìž…ì˜ ìŠ¤íŠ¸ë¦¼ë§Œ í•„í„°ë§\\n    for stream in yt.streams.filter(only_audio=True):\\n        mime_type = stream.mime_type\\n        abr = stream.abr\\n        if mime_type == \"audio/mp4\":\\n            abr = extract_abr(abr)\\n            mp4_files[abr] = stream\\n\\n    # í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\\n    sorted_keys = sorted(mp4_files.keys())\\n    # ê°€ìž¥ í° í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°’ ê°€ì ¸ì˜¤ê¸°\\n    largest_value = mp4_files[sorted_keys[-1]]\\n    filename = largest_value.download()\\n\\n    # í˜„ìž¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ì ˆëŒ€ ê²½ë¡œ ì–»ê¸°\\n    current_directory = os.path.abspath(\"\")\\n\\n    new_filename = os.path.basename(filename.replace(\".mp4\", \".wav\"))\\n\\n    new_filepath = os.path.join(current_directory, \"audio\", new_filename)\\n\\n    # mp4 íŒŒì¼ì„ wav íŒŒì¼ë¡œ ë³€í™˜\\n    convert_mp4_to_wav(filename, new_filepath)\\n\\n    # mp4 íŒŒì¼ ì‚­ì œ\\n    os.remove(filename)\\n    return new_filepath\\n\\n\\ndef extract_audio_from_video(video_filepath):\\n    # MP4 íŒŒì¼ ë¡œë“œ\\n    video = VideoFileClip(video_filepath)\\n    audio_filepath = get_audio_filepath(video_filepath.replace(\".mp4\", \".wav\"))\\n    video.audio.write_audiofile(audio_filepath)\\n    return audio_filepath\\n\\n\\nclass AudioChunk:\\n    def __init__(self, filepath, min_silence_len=350, silence_thresh=-35):\\n        self.audio = AudioSegment.from_file(filepath, format=\"wav\")\\n        self.filepath = filepath\\n        self.min_silence_len = min_silence_len\\n        self.silence_thresh = silence_thresh\\n        self.detect_nonsilent_from_audio()\\n\\n    @staticmethod\\n    def make_audio_chunks(audio, non_silent_times):\\n        audio_chunks = []\\n        for start, end in non_silent_times:\\n            audio_chunks.append((audio[start:end], start, end))\\n        return audio_chunks\\n\\n    def detect_nonsilent_from_audio(self):\\n        non_silent_audio_times = detect_nonsilent(\\n            self.audio,\\n            min_silence_len=self.min_silence_len,\\n            silence_thresh=self.silence_thresh,\\n        )\\n\\n        non_silent_audios_output = AudioSegment.empty()\\n\\n        for i in range(len(non_silent_audio_times)):\\n            non_silent_audios_output += self.audio[\\n                non_silent_audio_times[i][0] : non_silent_audio_times[i][1]\\n            ]\\n        self.audio_chunks = self.make_audio_chunks(self.audio, non_silent_audio_times)\\n        self.non_silent_audios_output = non_silent_audios_output\\n        print(f\"ë¶„ì„ì— ì‚¬ìš©í•  ì „ì²´ ì˜¤ë””ì˜¤ ì¡°ê° ê°œìˆ˜: {len(non_silent_audio_times)}\")\\n\\n    def audio_splits(self, split_time=100):\\n        splits = int(self.audio.duration_seconds // split_time + 1)\\n        audios = []\\n        for s in range(splits):\\n            start = s * split_time * 1000\\n            end = start + split_time * 1000\\n            audios.append(self.audio[start:end])\\n        return audios\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\12-RAG\\\\data\\\\audio_utils.py'}, page_content='import re\\nimport os\\nfrom pytube import YouTube\\nfrom moviepy.editor import AudioFileClip, VideoFileClip\\nfrom pydub import AudioSegment\\nfrom pydub.silence import detect_nonsilent\\n\\n\\ndef extract_abr(abr):\\n    youtube_audio_pattern = re.compile(r\"\\\\d+\")\\n    kbps = youtube_audio_pattern.search(abr)\\n    if kbps:\\n        kbps = kbps.group()\\n        return int(kbps)\\n    else:\\n        return 0\\n\\n\\ndef get_audio_filepath(filename):\\n    # audio í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\\n    if not os.path.isdir(\"audio\"):\\n        os.mkdir(\"audio\")\\n\\n    # í˜„ìž¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ì ˆëŒ€ ê²½ë¡œ ì–»ê¸°\\n    current_directory = os.path.abspath(\"\")\\n\\n    # íŒŒì¼ ê²½ë¡œ ìƒì„±\\n    audio_file_path = os.path.join(current_directory, \"audio\", filename)\\n\\n    return audio_file_path\\n\\n\\ndef convert_mp4_to_wav(mp4_file_path, wav_file_path):\\n    # MP4 íŒŒì¼ ë¡œë“œ\\n    audio_clip = AudioFileClip(mp4_file_path)\\n\\n    # WAV í˜•ì‹ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° ì €ìž¥\\n    audio_clip.write_audiofile(wav_file_path, fps=44100, nbytes=2, codec=\"pcm_s16le\")\\n\\n\\ndef download_audio_from_youtube(link):\\n    # YouTube ê°ì²´ ìƒì„±\\n    yt = YouTube(link)\\n\\n    # mp4 ì˜¤ë””ì˜¤ë§Œ í•„í„°ë§\\n    mp4_files = dict()\\n\\n    # \"audio/mp4\" íƒ€ìž…ì˜ ìŠ¤íŠ¸ë¦¼ë§Œ í•„í„°ë§\\n    for stream in yt.streams.filter(only_audio=True):\\n        mime_type = stream.mime_type\\n        abr = stream.abr\\n        if mime_type == \"audio/mp4\":\\n            abr = extract_abr(abr)\\n            mp4_files[abr] = stream\\n\\n    # í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\\n    sorted_keys = sorted(mp4_files.keys())\\n    # ê°€ìž¥ í° í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°’ ê°€ì ¸ì˜¤ê¸°\\n    largest_value = mp4_files[sorted_keys[-1]]\\n    filename = largest_value.download()\\n\\n    # í˜„ìž¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ì ˆëŒ€ ê²½ë¡œ ì–»ê¸°\\n    current_directory = os.path.abspath(\"\")\\n\\n    new_filename = os.path.basename(filename.replace(\".mp4\", \".wav\"))\\n\\n    new_filepath = os.path.join(current_directory, \"audio\", new_filename)\\n\\n    # mp4 íŒŒì¼ì„ wav íŒŒì¼ë¡œ ë³€í™˜\\n    convert_mp4_to_wav(filename, new_filepath)\\n\\n    # mp4 íŒŒì¼ ì‚­ì œ\\n    os.remove(filename)\\n    return new_filepath\\n\\n\\ndef extract_audio_from_video(video_filepath):\\n    # MP4 íŒŒì¼ ë¡œë“œ\\n    video = VideoFileClip(video_filepath)\\n    audio_filepath = get_audio_filepath(video_filepath.replace(\".mp4\", \".wav\"))\\n    video.audio.write_audiofile(audio_filepath)\\n    return audio_filepath\\n\\n\\nclass AudioChunk:\\n    def __init__(self, filepath, min_silence_len=350, silence_thresh=-35):\\n        self.audio = AudioSegment.from_file(filepath, format=\"wav\")\\n        self.filepath = filepath\\n        self.min_silence_len = min_silence_len\\n        self.silence_thresh = silence_thresh\\n        self.detect_nonsilent_from_audio()\\n\\n    @staticmethod\\n    def make_audio_chunks(audio, non_silent_times):\\n        audio_chunks = []\\n        for start, end in non_silent_times:\\n            audio_chunks.append((audio[start:end], start, end))\\n        return audio_chunks\\n\\n    def detect_nonsilent_from_audio(self):\\n        non_silent_audio_times = detect_nonsilent(\\n            self.audio,\\n            min_silence_len=self.min_silence_len,\\n            silence_thresh=self.silence_thresh,\\n        )\\n\\n        non_silent_audios_output = AudioSegment.empty()\\n\\n        for i in range(len(non_silent_audio_times)):\\n            non_silent_audios_output += self.audio[\\n                non_silent_audio_times[i][0] : non_silent_audio_times[i][1]\\n            ]\\n        self.audio_chunks = self.make_audio_chunks(self.audio, non_silent_audio_times)\\n        self.non_silent_audios_output = non_silent_audios_output\\n        print(f\"ë¶„ì„ì— ì‚¬ìš©í•  ì „ì²´ ì˜¤ë””ì˜¤ ì¡°ê° ê°œìˆ˜: {len(non_silent_audio_times)}\")\\n\\n    def audio_splits(self, split_time=100):\\n        splits = int(self.audio.duration_seconds // split_time + 1)\\n        audios = []\\n        for s in range(splits):\\n            start = s * split_time * 1000\\n            end = start + split_time * 1000\\n            audios.append(self.audio[start:end])\\n        return audios\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\16-Use-Cases\\\\telegram-langchain-bot\\\\telegram-langchain-bot.py'}, page_content='import os\\nfrom dotenv import load_dotenv\\nfrom langchain_text_splitters import Language\\nfrom langchain_community.document_loaders.generic import GenericLoader\\nfrom langchain_community.document_loaders.parsers import LanguageParser\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\nload_dotenv()\\n\\n# Python íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\\nrepo_root = \"/home/hellocosmos/telegram-bot/langchain/libs\"\\nrepo_core = repo_root + \"/core/langchain_core\"\\nrepo_community = repo_root + \"/community/langchain_community\"\\nrepo_experimental = repo_root + \"/experimental/langchain_experimental\"\\nrepo_partners = repo_root + \"/partners\"\\nrepo_text_splitter = repo_root + \"/text_splitters/langchain_text_splitters\"\\nrepo_cookbook = repo_root + \"/cookbook\"\\n\\npy_documents = []\\nfor path in [repo_core, repo_community, repo_experimental, repo_partners, repo_cookbook]:\\n    loader = GenericLoader.from_filesystem(\\n        path, glob=\"**/*\", suffixes=[\".py\"],\\n        parser=LanguageParser(language=Language.PYTHON, parser_threshold=30),\\n    )\\n    py_documents.extend(loader.load())\\nprint(f\".py íŒŒì¼ì˜ ê°œìˆ˜: {len(py_documents)}\")\\n\\npy_splitter = RecursiveCharacterTextSplitter.from_language(\\n    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\\n)\\npy_docs = py_splitter.split_documents(py_documents)\\nprint(f\"ë¶„í• ëœ .py íŒŒì¼ì˜ ê°œìˆ˜: {len(py_docs)}\")\\n\\n# MDX íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\\nroot_dir = \"/home/hellocosmos/telegram-bot/langchain/\"\\n\\nmdx_documents = []\\nfor dirpath, dirnames, filenames in os.walk(root_dir):\\n    for file in filenames:\\n        if (file.endswith(\".mdx\")) and \"*venv/\" not in dirpath:\\n            try:\\n                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\\n                mdx_documents.extend(loader.load())\\n            except Exception:\\n                pass\\nprint(f\".mdx íŒŒì¼ì˜ ê°œìˆ˜: {len(mdx_documents)}\")\\n\\nmdx_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\\nmdx_docs = mdx_splitter.split_documents(mdx_documents)\\nprint(f\"ë¶„í• ëœ .mdx íŒŒì¼ì˜ ê°œìˆ˜: {len(mdx_docs)}\")\\n\\n# Teddyë‹˜ì˜ ëž­ì²´ì¸ë…¸íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\\nimport pandas as pd\\nfrom langchain.schema import Document\\n\\ndf = pd.read_csv(\\'data_list_with_content.csv\\')\\ndf_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\\nteddy_docs = []\\nfor index, row in df.iterrows():\\n    if pd.isna(row[\\'content\\']):\\n        continue\\n    chunks = df_splitter.split_text(row[\\'content\\'])\\n    for chunk in chunks:\\n        teddy_docs.append(Document(page_content=chunk, metadata={\"title\": row[\\'title\\'], \"source\": row[\\'source\\']}))\\nprint(f\"ë¶„í• ëœ .df íŒŒì¼ ê°œìˆ˜: {len(teddy_docs)}\")\\n\\n# PDF íŒŒì¼ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• í•©ë‹ˆë‹¤. (PDF íŒŒì¼ì€ ìœ ë£Œêµ¬ë§¤í•˜ì…”ì•¼ í•©ë‹ˆë‹¤)\\npdf_docs = []\\ndocument = PyPDFLoader(\"data/Generative_Al_with_LangChain.pdf\").load_and_split()\\npdf_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\\npdf_docs = pdf_splitter.split_documents(document)\\nprint(f\"ë¶„í• ëœ .pdf íŒŒì¼ì˜ ê°œìˆ˜: {len(pdf_docs)}\")\\n\\n# íŒŒì´ì¬ ë¬¸ì„œ, MDX ë¬¸ì„œ, PDF ë¬¸ì„œ, í…Œë””ë…¸íŠ¸(Langhchin-KR) ë¬¸ì„œë¥¼ ê²°í•©í•©ë‹ˆë‹¤.\\ncombined_documents = teddy_docs + py_docs + mdx_docs + pdf_docs\\nprint(f\"ì´ ë„íë¨¼íŠ¸ ê°œìˆ˜: {len(combined_documents)}\")\\n\\n# í•„ìš”í•œ ìž„ë² ë”©ê³¼ ìºì‹±ì„¤ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. \\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain.embeddings import CacheBackedEmbeddings\\nfrom langchain.storage import LocalFileStore\\n\\nstore = LocalFileStore(\"./cache/\")\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", disallowed_special=())\\ncached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, store, namespace=embeddings.model)\\n\\n# Kiwi Tokenizerë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\\nfrom kiwipiepy import Kiwi\\n\\nkiwi = Kiwi()\\ndef kiwi_tokenize(text):\\n    return [token.form for token in kiwi.tokenize(text)]\\n\\n# FAISS í´ëž˜ìŠ¤ë¥¼ ê°€ì ¸ì™€ ê²€ìƒ‰ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\\nfrom langchain_community.vectorstores import FAISS, Chroma\\n\\nFAISS_DB_INDEX = \"langchain_faiss\"\\ndb = FAISS.from_documents(combined_documents, cached_embeddings)\\ndb.save_local(folder_path=FAISS_DB_INDEX)\\ndb = FAISS.load_local(FAISS_DB_INDEX, cached_embeddings, allow_dangerous_deserialization=True)\\nfaiss_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})\\n\\n# BM25Retriever í´ëž˜ìŠ¤ë¥¼ ê°€ì ¸ì™€ ê²€ìƒ‰ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\\nfrom langchain_community.retrievers import BM25Retriever\\n\\nkiwi_bm25_retriever = BM25Retriever.from_documents(combined_documents, preprocess_func=kiwi_tokenize, k=10)\\n\\n# EnsembleRetriever í´ëž˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.\\nfrom langchain.retrievers import EnsembleRetriever\\n\\nensemble_retriever = EnsembleRetriever(\\n    retrievers=[kiwi_bm25_retriever, faiss_retriever],\\n    weights=[0.7, 0.3], search_type=\"mmr\",\\n)\\n\\n# PromptTemplateì„ ìƒì„±í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\\nfrom langchain_core.prompts import PromptTemplate\\n\\nprompt = PromptTemplate.from_template(\\n\"\"\"\\në‹¹ì‹ ì€ 20ë…„ì°¨ AI ê°œë°œìžì´ìž íŒŒì´ì¬ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ìž„ë¬´ëŠ” ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ ìµœëŒ€í•œ ë¬¸ì„œì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤. ì•„ëž˜ì˜ ìˆ«ìžê°€ ì ížŒ ìˆœì„œëŒ€ë¡œ ì ˆì°¨ë¥¼ ì§€ì¼œì„œ ë‹¨ê³„ì ìœ¼ë¡œ ìƒê°í•˜ê³  ì§„í–‰í•˜ì„¸ìš”.\\n\\n1.ì£¼ì–´ì§„ ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ëŠ” ê²½ìš°, \"ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤\" ë¼ê³  ì‹œìž‘í•œë‹¤. Python ì½”ë“œì— ëŒ€í•œ ìƒì„¸í•œ code snippetì„ í¬í•¨í•´ì•¼ í•˜ë©°, ì½”ë“œ ì„¤ëª…ì— ëŒ€í•œ ì£¼ì„ë„ ìž‘ì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ìžì„¸í•˜ê²Œ ì„¤ëª…í•˜ê³ , í•œê¸€ë¡œ ìž‘ì„±í•´ ì£¼ì„¸ìš”. ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ëŠ” ê²½ìš° ì¶œì²˜(source)ë¥¼ ë°˜ë“œì‹œ í‘œê¸°í•´ì•¼ í•©ë‹ˆë‹¤. ì¶œì²˜ëŠ” ì ˆëŒ€ê²½ë¡œë¡œ ì¶œë ¥ë˜ëŠ” ê²½ìš° \"/home/hellocosmos/telegram-bot\"ì€ ìƒëžµí•˜ê³  ì¶œë ¥í•´ì£¼ì„¸ìš”. ì¶œì²˜ê°€ PDF íŒŒì¼ì¸ ê²½ìš° \"ì¶œì²˜ ì†ŒìŠ¤, íŽ˜ì´ì§€\"ë¥¼ í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•´ì•¼ í•©ë‹ˆë‹¤. ë©”íƒ€ë°ì´í„°ì˜ titleì´ ë¹ˆê³µë°±ì´ ì•„ë‹Œ ê²½ìš° ë°˜ë“œì‹œ \"title, source\" í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•´ì•¼ í•©ë‹ˆë‹¤.\\n2.ì£¼ì–´ì§„ ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°ì—ëŠ” AI, Langchain ë° íŒŒì´ì¬ ì „ë¬¸ê°€ë¡œì¨ ë‹¹ì‹ ì´ ì•Œê³  ìžˆëŠ” ê´€ë ¨ ì§€ì‹ë§Œì„ í™œìš©í•´ì•¼ í•©ë‹ˆë‹¤. \"ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ì§€ë§Œ, ì•Œê³  ìžˆëŠ” ì§€ì‹ì„ í™œìš©í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\"ë¼ê³  ì‹œìž‘í•œë‹¤. ìµœëŒ€í•œ ìžì„¸í•˜ê²Œ ë‹µë³€í•˜ê³ , ì¶œì²˜ëŠ” ë‹¹ì‹ ì´ ì•Œê³  ìžˆëŠ” ì¶œì²˜ë¥¼ í‘œê¸°í•´ì£¼ì„¸ìš”.\\n3.ì£¼ì–´ì§„ ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ê³ , Langchain ë° íŒŒì´ì¬ ì „ë¬¸ê°€ë¡œì¨ ë‹¹ì‹ ì´ ì•Œê³  ìžˆëŠ” ê´€ë ¨ ì§€ì‹ë§Œì„ í™œìš©í•´ë„ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” \"AI ë° Langchainì— ëŒ€í•´ ë¬¸ì˜í•´ì£¼ì„¸ìš” ðŸ˜‚\"ë¼ê³  ë‹µë³€í•´ì•¼ í•˜ë©°, ì¶œì²˜ëŠ” ìƒëžµ í•´ì£¼ì„¸ìš”.\\n\\n#ì°¸ê³ ë¬¸ì„œ:\\n{context}\\n\\n#ì§ˆë¬¸:\\n{question}\\n\\n#ë‹µë³€: \\n\\n#ì¶œì²˜:\\n- source1\\n- source2\\n- ...\\n\"\"\"\\n)\\n\\nfrom langchain.callbacks.base import BaseCallbackHandler\\nfrom langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\nfrom langchain_core.callbacks.manager import CallbackManager\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI\\n\\nclass StreamCallback(BaseCallbackHandler):\\n    def on_llm_new_token(self, token: str, **kwargs):\\n        print(token, end=\"\", flush=True)\\n\\n# LLM ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤.\\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0, streaming=True, callbacks=[StreamCallback()])\\n\\n# Retriever ë¬¸ì„œë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤.\\ndef format_docs(documents):\\n    formatted_list = []\\n    for doc in documents:\\n        title = doc.metadata.get(\\'title\\', \\'\\')  # titleì´ ìžˆìœ¼ë©´ ê°€ì ¸ì˜¤ê³ , ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìžì—´ë¡œ ì„¤ì •\\n        formatted_list.append(\\n            f\"<doc><content>{doc.page_content}</content><title>{title}</title><source>{doc.metadata[\\'source\\']}</source></doc>\"\\n        )\\n    return formatted_list\\n\\n# ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\\nrag_chain = (\\n    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\\n    | prompt | llm | StrOutputParser()\\n)\\n\\n# ë‚ ì§œì™€ ì‹œê°„ í•¨ìˆ˜\\nfrom datetime import datetime\\n\\ndef get_current_datetime():\\n    now = datetime.now()\\n    formatted_datetime = now.strftime(\"%Y-%m-%d %H:%M:%S\")\\n    return formatted_datetime\\n\\n# í…”ë ˆê·¸ëž¨ ë´‡ ì„¤ì • ë° í•¸ë“¤ëŸ¬ ì •ì˜\\nimport telegram\\nfrom telegram.ext import Application, CommandHandler, MessageHandler, filters\\nfrom telegram.constants import ChatAction, ParseMode\\n\\n# í…”ë ˆê·¸ëž¨ ë´‡ í† í°ì„ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\\nbot = telegram.Bot(os.getenv(\"BOT_TOKEN\"))\\n\\n# RAG ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\\ndef generate_response(message):\\n    return rag_chain.invoke(message)\\n\\n# í…ìŠ¤íŠ¸ë¥¼ Telegram Markdown V2 í˜•ì‹ìœ¼ë¡œ ì´ìŠ¤ì¼€ì´í”„í•˜ëŠ” í•¨ìˆ˜\\ndef escape_markdown_v2(text):\\n    escape_chars = r\\'\\\\`*_{}[]()#+-.!|>=\\'\\n    return \\'\\'.join([\\'\\\\\\\\\\' + char if char in escape_chars else char for char in text])\\n\\n# ì‘ë‹µì„ ë‚˜ëˆ„ì–´ ë§ˆí¬ë‹¤ìš´ V2 í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜\\ndef split_response(response):\\n    parts = response.split(\"```\")\\n    result = []\\n    for i, part in enumerate(parts):\\n        if i % 2 == 0:\\n            result.append(escape_markdown_v2(part))\\n        else:\\n            result.append(f\"```{part}```\")\\n    return result\\n\\n# ë´‡ì˜ /start ëª…ë ¹ì— ëŒ€í•œ í•¸ë“¤ëŸ¬ í•¨ìˆ˜\\nasync def start(update, context):\\n    await context.bot.send_message(chat_id=update.effective_chat.id, text=\"ì•ˆë…•í•˜ì„¸ìš”, Langchain ì±—ë´‡ìž…ë‹ˆë‹¤! ðŸ§‘\\u200dðŸ’»\")\\n\\n# í…”ë ˆê·¸ëž¨ ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¸ë“¤ëŸ¬ í•¨ìˆ˜\\nasync def answer_openai(update, context):\\n    message = update.message.text\\n\\n    # ìœ ì € ì´ë¦„ ë˜ëŠ” ì‚¬ìš©ìžëª… ì¶”ì¶œ\\n    user = update.message.from_user  # ìœ ì € ì •ë³´ ì¶”ì¶œ\\n    user_id = update.message.from_user.id  # ìœ ì € ID ì¶”ì¶œ\\n    user_identifier = user.username if user.username else f\"{user.first_name} {user.last_name if user.last_name else \\'\\'}\"\\n    date_time = get_current_datetime()\\n    print(f\"\\\\n[User_Info] uid: {user_id},  name: {user_identifier}, date: {date_time}\")\\n    print(f\"\\\\n[Question] {message}\\\\n[Answer]\\\\n\")    \\n\\n    chat_id = update.effective_chat.id\\n\\n    loading_message = await context.bot.send_message(chat_id=chat_id, text=\"ì²˜ë¦¬ ì¤‘ìž…ë‹ˆë‹¤... ðŸ§‘\\u200dðŸ’»\")\\n    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)\\n    try:\\n        response = generate_response(message)\\n        print(\"\\\\n\\\\n\")\\n    except Exception as e:\\n        await context.bot.delete_message(chat_id=chat_id, message_id=loading_message.message_id)\\n        await context.bot.send_message(chat_id=chat_id, text=f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\\n        return\\n    \\n    await context.bot.delete_message(chat_id=chat_id, message_id=loading_message.message_id)\\n    \\n    # ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ê³  ë§ˆí¬ë‹¤ìš´ V2 ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬\\n    formatted_response_parts = split_response(response)\\n    \\n    # ë””ë²„psê¹… ì¶œë ¥ì„ ì¶”ê°€í•˜ì—¬ ì´ìŠ¤ì¼€ì´í”„ëœ í…ìŠ¤íŠ¸ í™•ì¸\\n    # for part in formatted_response_parts: print(part)\\n    \\n    # ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¸¸ë©´ ë‚˜ëˆ„ì–´ì„œ ë³´ë‚´ê¸°\\n    for part in formatted_response_parts:\\n        if part.strip():  # partê°€ ë¹„ì–´ìžˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ë©”ì‹œì§€ ì „ì†¡\\n            await context.bot.send_message(chat_id=update.effective_chat.id, text=part, parse_mode=ParseMode.MARKDOWN_V2)\\n# í…”ë ˆê·¸ëž¨ ë´‡ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± ë° í•¸ë“¤ëŸ¬ ì¶”ê°€\\n\\napplication = Application.builder().token(os.getenv(\"BOT_TOKEN\")).build()\\napplication.add_handler(CommandHandler(\\'start\\', start))\\napplication.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, answer_openai))\\n\\n# ë´‡ ì‹¤í–‰\\napplication.run_polling()'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\17-LangGraph\\\\rag\\\\base.py'}, page_content='from langchain import hub\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_upstage import UpstageEmbeddings\\nfrom langchain_openai import ChatOpenAI\\n\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom abc import ABC, abstractmethod\\nfrom operator import itemgetter\\n\\n\\nclass RetrievalChain(ABC):\\n    def __init__(self):\\n        self.source_uri = None\\n        self.k = 5\\n\\n    @abstractmethod\\n    def load_documents(self, source_uris):\\n        \"\"\"loaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def create_text_splitter(self):\\n        \"\"\"text splitterë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\\n        pass\\n\\n    def split_documents(self, docs, text_splitter):\\n        \"\"\"text splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\"\"\"\\n        return text_splitter.split_documents(docs)\\n\\n    def create_embedding(self):\\n        return UpstageEmbeddings(model=\"solar-embedding-1-large\")\\n\\n    def create_vectorstore(self, split_docs):\\n        return FAISS.from_documents(\\n            documents=split_docs, embedding=self.create_embedding()\\n        )\\n\\n    def create_retriever(self, vectorstore):\\n        # MMRì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\\n        dense_retriever = vectorstore.as_retriever(\\n            search_type=\"mmr\", search_kwargs={\"k\": self.k}\\n        )\\n        return dense_retriever\\n\\n    def create_model(self):\\n        return ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0)\\n\\n    def create_prompt(self):\\n        return hub.pull(\"teddynote/rag-korean-with-source\")\\n\\n    @staticmethod\\n    def format_docs(docs):\\n        return \"\\\\n\".join(docs)\\n\\n    def create_chain(self):\\n        docs = self.load_documents(self.source_uri)\\n        text_splitter = self.create_text_splitter()\\n        split_docs = self.split_documents(docs, text_splitter)\\n        self.vectorstore = self.create_vectorstore(split_docs)\\n        self.retriever = self.create_retriever(self.vectorstore)\\n        model = self.create_model()\\n        prompt = self.create_prompt()\\n        self.chain = (\\n            {\"question\": itemgetter(\"question\"), \"context\": itemgetter(\"context\")}\\n            | prompt\\n            | model\\n            | StrOutputParser()\\n        )\\n        return self\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\17-LangGraph\\\\rag\\\\pdf.py'}, page_content='from rag.base import RetrievalChain\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom typing import List\\n\\n\\nclass PDFRetrievalChain(RetrievalChain):\\n    def __init__(self, source_uri):\\n        self.source_uri = source_uri\\n        self.k = 5\\n\\n    def load_documents(self, source_uris: List[str]):\\n        docs = []\\n        for source_uri in source_uris:\\n            loader = PDFPlumberLoader(source_uri)\\n            docs.extend(loader.load())\\n\\n        return docs\\n\\n    def create_text_splitter(self):\\n        return RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\17-LangGraph\\\\rag\\\\utils.py'}, page_content='def format_docs(docs):\\n    return \"\\\\n\".join(\\n        [\\n            f\"<document><content>{doc.page_content}</content><source>{doc.metadata[\\'source\\']}</source><page>{int(doc.metadata[\\'page\\'])+1}</page></document>\"\\n            for doc in docs\\n        ]\\n    )\\n\\n\\ndef format_searched_docs(docs):\\n    return \"\\\\n\".join(\\n        [\\n            f\"<document><content>{doc[\\'content\\']}</content><source>{doc[\\'url\\']}</source></document>\"\\n            for doc in docs\\n        ]\\n    )\\n\\n\\ndef format_task(tasks):\\n    # ê²°ê³¼ë¥¼ ì €ìž¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±\\n    task_time_pairs = []\\n\\n    # ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©´ì„œ ê° í•­ëª©ì„ ì²˜ë¦¬\\n    for item in tasks:\\n        # ì½œë¡ (:) ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìžì—´ì„ ë¶„ë¦¬\\n        task, time_str = item.rsplit(\":\", 1)\\n        # \\'ì‹œê°„\\' ë¬¸ìžì—´ì„ ì œê±°í•˜ê³  ì •ìˆ˜ë¡œ ë³€í™˜\\n        time = int(time_str.replace(\"ì‹œê°„\", \"\").strip())\\n        # í•  ì¼ê³¼ ì‹œê°„ì„ íŠœí”Œë¡œ ë§Œë“¤ì–´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\\n        task_time_pairs.append((task, time))\\n\\n    # ê²°ê³¼ ì¶œë ¥\\n    return task_time_pairs\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\00-Chat-Template\\\\chat_history.py'}, page_content='import streamlit as st\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import load_prompt\\n\\n\\nst.set_page_config(page_title=\"ë‚˜ë§Œì˜ ChatGPT ðŸ’¬\", page_icon=\"ðŸ’¬\")\\nst.title(\"ë‚˜ë§Œì˜ ChatGPT ðŸ’¬\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n\\ndef print_history():\\n    for msg in st.session_state[\"messages\"]:\\n        st.chat_message(msg.role).write(msg.content)\\n\\n\\ndef add_history(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\n# ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\\ndef create_chain(prompt, model):\\n    chain = prompt | ChatOpenAI(model_name=model) | StrOutputParser()\\n    return chain\\n\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”\")\\n    tab1, tab2 = st.tabs([\"í”„ë¡¬í”„íŠ¸\", \"í”„ë¦¬ì…‹\"])\\n    prompt = \"\"\"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\"\"\\n    user_text_prompt = tab1.text_area(\"í”„ë¡¬í”„íŠ¸\", value=prompt)\\n    user_text_apply_btn = tab1.button(\"í”„ë¡¬í”„íŠ¸ ì ìš©\", key=\"apply1\")\\n    if user_text_apply_btn:\\n        tab1.markdown(f\"âœ… í”„ë¡¬í”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤\")\\n        prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n        prompt = PromptTemplate.from_template(prompt_template)\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\n    user_selected_prompt = tab2.selectbox(\"í”„ë¦¬ì…‹ ì„ íƒ\", [\"sns\", \"ë²ˆì—­\", \"ìš”ì•½\"])\\n    user_selected_apply_btn = tab2.button(\"í”„ë¡¬í”„íŠ¸ ì ìš©\", key=\"apply2\")\\n    if user_selected_apply_btn:\\n        tab2.markdown(f\"âœ… í”„ë¡¬í”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤\")\\n        prompt = load_prompt(f\"prompts/{user_selected_prompt}.yaml\", encoding=\"utf8\")\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\nif clear_btn:\\n    retriever = st.session_state[\"messages\"].clear()\\n\\nprint_history()\\n\\n\\nif \"chain\" not in st.session_state:\\n    # user_prompt\\n    prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n    prompt = PromptTemplate.from_template(prompt_template)\\n    st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\nif user_input := st.chat_input():\\n    add_history(\"user\", user_input)\\n    st.chat_message(\"user\").write(user_input)\\n    with st.chat_message(\"assistant\"):\\n        chat_container = st.empty()\\n\\n        stream_response = st.session_state[\"chain\"].stream(\\n            {\"question\": user_input}\\n        )  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜\\n        ai_answer = \"\"\\n        for chunk in stream_response:\\n            ai_answer += chunk\\n            chat_container.markdown(ai_answer)\\n        add_history(\"ai\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\00-Chat-Template\\\\main.py'}, page_content='import streamlit as st\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import load_prompt\\n\\n\\nst.set_page_config(page_title=\"ë‚˜ë§Œì˜ ChatGPT ðŸ’¬\", page_icon=\"ðŸ’¬\")\\nst.title(\"ë‚˜ë§Œì˜ ChatGPT ðŸ’¬\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n\\ndef print_history():\\n    for msg in st.session_state[\"messages\"]:\\n        st.chat_message(msg.role).write(msg.content)\\n\\n\\ndef add_history(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\n# ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\\ndef create_chain(prompt, model):\\n    chain = prompt | ChatOpenAI(model_name=model) | StrOutputParser()\\n    return chain\\n\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”\")\\n    tab1, tab2 = st.tabs([\"í”„ë¡¬í”„íŠ¸\", \"í”„ë¦¬ì…‹\"])\\n    prompt = \"\"\"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\"\"\\n    user_text_prompt = tab1.text_area(\"í”„ë¡¬í”„íŠ¸\", value=prompt)\\n    user_text_apply_btn = tab1.button(\"í”„ë¡¬í”„íŠ¸ ì ìš©\", key=\"apply1\")\\n    if user_text_apply_btn:\\n        tab1.markdown(f\"âœ… í”„ë¡¬í”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤\")\\n        prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n        prompt = PromptTemplate.from_template(prompt_template)\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\n    user_selected_prompt = tab2.selectbox(\"í”„ë¦¬ì…‹ ì„ íƒ\", [\"sns\", \"ë²ˆì—­\", \"ìš”ì•½\"])\\n    user_selected_apply_btn = tab2.button(\"í”„ë¡¬í”„íŠ¸ ì ìš©\", key=\"apply2\")\\n    if user_selected_apply_btn:\\n        tab2.markdown(f\"âœ… í”„ë¡¬í”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤\")\\n        prompt = load_prompt(f\"prompts/{user_selected_prompt}.yaml\", encoding=\"utf8\")\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\nif clear_btn:\\n    retriever = st.session_state[\"messages\"].clear()\\n\\nprint_history()\\n\\n\\nif \"chain\" not in st.session_state:\\n    # user_prompt\\n    prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n    prompt = PromptTemplate.from_template(prompt_template)\\n    st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\nif user_input := st.chat_input():\\n    add_history(\"user\", user_input)\\n    st.chat_message(\"user\").write(user_input)\\n    with st.chat_message(\"assistant\"):\\n        chat_container = st.empty()\\n\\n        stream_response = st.session_state[\"chain\"].stream(\\n            {\"question\": user_input}\\n        )  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜\\n        ai_answer = \"\"\\n        for chunk in stream_response:\\n            ai_answer += chunk\\n            chat_container.markdown(ai_answer)\\n        add_history(\"ai\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\01-MyProject\\\\retriever.py'}, page_content='from langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import OpenAIEmbeddings\\n\\n\\ndef create_retriever(file_path):\\n    # ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ(Load Documents)\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í• (Split Documents)\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    split_documents = text_splitter.split_documents(docs)\\n\\n    # ë‹¨ê³„ 3: ìž„ë² ë”©(Embedding) ìƒì„±\\n    embeddings = OpenAIEmbeddings()\\n\\n    # ë‹¨ê³„ 4: DB ìƒì„±(Create DB) ë° ì €ìž¥\\n    # ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\\n    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\\n\\n    # ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±\\n    # ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìžˆëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.\\n    retriever = vectorstore.as_retriever()\\n    return retriever\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\01-MyProject\\\\pages\\\\01_PDF.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_teddynote import logging\\nfrom dotenv import load_dotenv\\nimport os\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n\\n# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ìž…ë ¥í•©ë‹ˆë‹¤.\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDF ê¸°ë°˜ QAðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    # ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•Šì„ ê²½ìš°\\n    st.session_state[\"chain\"] = None\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"íŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ì„ íƒ ë©”ë‰´\\n    selected_model = st.selectbox(\\n        \"LLM ì„ íƒ\", [\"gpt-4o\", \"gpt-4-turbo\", \"gpt-4o-mini\"], index=0\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ìž…ë‹ˆë‹¤...\")\\ndef embed_file(file):\\n    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥í•©ë‹ˆë‹¤.\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ(Load Documents)\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í• (Split Documents)\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    split_documents = text_splitter.split_documents(docs)\\n\\n    # ë‹¨ê³„ 3: ìž„ë² ë”©(Embedding) ìƒì„±\\n    embeddings = OpenAIEmbeddings()\\n\\n    # ë‹¨ê³„ 4: DB ìƒì„±(Create DB) ë° ì €ìž¥\\n    # ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\\n    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\\n\\n    # ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±\\n    # ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìžˆëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.\\n    retriever = vectorstore.as_retriever()\\n    return retriever\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(retriever, model_name=\"gpt-4o\"):\\n    # ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)\\n    # í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±\\n    # ëª¨ë¸(LLM) ì„ ìƒì„±í•©ë‹ˆë‹¤.\\n    llm = ChatOpenAI(model_name=model_name, temperature=0)\\n\\n    # ë‹¨ê³„ 8: ì²´ì¸(Chain) ìƒì„±\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ\\nif uploaded_file:\\n    # íŒŒì¼ ì—…ë¡œë“œ í›„ retriever ìƒì„± (ìž‘ì—…ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦´ ì˜ˆì •...)\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    # chain ì„ ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n    else:\\n        # íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\01-MyProject\\\\pages\\\\02_Local_RAG.py'}, page_content='from pyexpat import model\\nimport streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_teddynote import logging\\nfrom dotenv import load_dotenv\\nimport os\\nfrom retriever import create_retriever\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n\\n# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ìž…ë ¥í•©ë‹ˆë‹¤.\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"Local ëª¨ë¸ ê¸°ë°˜ RAG ðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    # ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•Šì„ ê²½ìš°\\n    st.session_state[\"chain\"] = None\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # íŒŒì¼ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"íŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    # ëª¨ë¸ ì„ íƒ ë©”ë‰´\\n    selected_model = st.selectbox(\"LLM ì„ íƒ\", [\"xionic\", \"ollama\"], index=0)\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# íŒŒì¼ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ìž…ë‹ˆë‹¤...\")\\ndef embed_file(file):\\n    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥í•©ë‹ˆë‹¤.\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    return create_retriever(file_path)\\n\\n\\ndef format_doc(document_list):\\n    return \"\\\\n\\\\n\".join([doc.page_content for doc in document_list])\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(retriever, model_name=\"xionic\"):\\n    # ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)\\n    # í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\\n    if model_name == \"xionic\":\\n        # ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)\\n        prompt = load_prompt(\"prompts/pdf-rag-xionic.yaml\", encoding=\"utf-8\")\\n\\n        # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±\\n        llm = ChatOpenAI(\\n            model_name=\"xionic-1-72b-20240610\",\\n            base_url=\"https://sionic.chat/v1/\",\\n            api_key=\"934c4bbc-c384-4bea-af82-1450d7f8128d\",\\n        )\\n    elif model_name == \"ollama\":\\n        # ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)\\n        prompt = load_prompt(\"prompts/pdf-rag-ollama.yaml\", encoding=\"utf-8\")\\n\\n        # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±\\n        # Ollama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\\n        llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\", temperature=0)\\n\\n    # ë‹¨ê³„ 8: ì²´ì¸(Chain) ìƒì„±\\n    chain = (\\n        {\"context\": retriever | format_doc, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ\\nif uploaded_file:\\n    # íŒŒì¼ ì—…ë¡œë“œ í›„ retriever ìƒì„± (ìž‘ì—…ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦´ ì˜ˆì •...)\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    # chain ì„ ìƒì„±\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n    else:\\n        # íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\01-MyProject\\\\pages\\\\03_Multi_Modal.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_teddynote import logging\\nfrom langchain_teddynote.models import MultiModal\\n\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n\\n# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ìž…ë ¥í•©ë‹ˆë‹¤.\\nlogging.langsmith(\"[Project] ì´ë¯¸ì§€ ì¸ì‹\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"ì´ë¯¸ì§€ ì¸ì‹ ê¸°ë°˜ ì±—ë´‡ ðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\n# íƒ­ì„ ìƒì„±\\nmain_tab1, main_tab2 = st.tabs([\"ì´ë¯¸ì§€\", \"ëŒ€í™”ë‚´ìš©\"])\\n\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # ì´ë¯¸ì§€ ì—…ë¡œë“œ\\n    uploaded_file = st.file_uploader(\"ì´ë¯¸ì§€ ì—…ë¡œë“œ\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n    # ëª¨ë¸ ì„ íƒ ë©”ë‰´\\n    selected_model = st.selectbox(\"LLM ì„ íƒ\", [\"gpt-4o\", \"gpt-4o-mini\"], index=0)\\n\\n    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€\\n    system_prompt = st.text_area(\\n        \"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\",\\n        \"ë‹¹ì‹ ì€ í‘œ(ìž¬ë¬´ì œí‘œ) ë¥¼ í•´ì„í•˜ëŠ” ê¸ˆìœµ AI ì–´ì‹œìŠ¤í„´íŠ¸ ìž…ë‹ˆë‹¤.\\\\në‹¹ì‹ ì˜ ìž„ë¬´ëŠ” ì£¼ì–´ì§„ í…Œì´ë¸” í˜•ì‹ì˜ ìž¬ë¬´ì œí‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ í¥ë¯¸ë¡œìš´ ì‚¬ì‹¤ì„ ì •ë¦¬í•˜ì—¬ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤.\",\\n        height=200,\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        main_tab2.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì´ë¯¸ì§€ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬ ì¤‘ìž…ë‹ˆë‹¤...\")\\ndef process_imagefile(file):\\n    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥í•©ë‹ˆë‹¤.\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    return file_path\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef generate_answer(image_filepath, system_prompt, user_prompt, model_name=\"gpt-4o\"):\\n    # ê°ì²´ ìƒì„±\\n    llm = ChatOpenAI(\\n        temperature=0,\\n        model_name=model_name,  # ëª¨ë¸ëª…\\n    )\\n\\n    # ë©€í‹°ëª¨ë‹¬ ê°ì²´ ìƒì„±\\n    multimodal = MultiModal(llm, system_prompt=system_prompt, user_prompt=user_prompt)\\n\\n    # ì´ë¯¸ì§€ íŒŒì¼ë¡œ ë¶€í„° ì§ˆì˜(ìŠ¤íŠ¸ë¦¼ ë°©ì‹)\\n    answer = multimodal.stream(image_filepath)\\n    return answer\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = main_tab2.empty()\\n\\n# ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œê°€ ëœë‹¤ë©´...\\nif uploaded_file:\\n    # ì´ë¯¸ì§€ íŒŒì¼ì„ ì²˜ë¦¬\\n    image_filepath = process_imagefile(uploaded_file)\\n    main_tab1.image(image_filepath)\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    # íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆëŠ”ì§€ í™•ì¸\\n    if uploaded_file:\\n        # ì´ë¯¸ì§€ íŒŒì¼ì„ ì²˜ë¦¬\\n        image_filepath = process_imagefile(uploaded_file)\\n        # ë‹µë³€ ìš”ì²­\\n        response = generate_answer(\\n            image_filepath, system_prompt, user_input, selected_model\\n        )\\n\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        main_tab2.chat_message(\"user\").write(user_input)\\n\\n        with main_tab2.chat_message(\"assistant\"):\\n            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token.content\\n                container.markdown(ai_answer)\\n\\n        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n    else:\\n        # ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥\\n        warning_msg.error(\"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\01-MyProject\\\\pages\\\\04_Multi_Turn.py'}, page_content='from urllib import response\\nfrom requests import session\\nimport streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_teddynote import logging\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\nfrom langchain_core.chat_history import BaseChatMessageHistory\\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n\\n# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ìž…ë ¥í•©ë‹ˆë‹¤.\\nlogging.langsmith(\"[Project] Multi Tuen ì±—ë´‡\")\\n\\n# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"ëŒ€í™”ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” ì±—ë´‡ ðŸ’¬\")\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # ëª¨ë¸ ì„ íƒ ë©”ë‰´\\n    selected_model = st.selectbox(\"LLM ì„ íƒ\", [\"gpt-4o\", \"gpt-4o-mini\"], index=0)\\n\\n    # ì„¸ì…˜ ID ë¥¼ ì§€ì •í•˜ëŠ” ë©”ë‰´\\n    session_id = st.text_input(\"ì„¸ì…˜ IDë¥¼ ìž…ë ¥í•˜ì„¸ìš”.\", \"abc123\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°\\n        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ìž¥\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(model_name=\"gpt-4o\"):\\n\\n    # í”„ë¡¬í”„íŠ¸ ì •ì˜\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ìž…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\",\\n            ),\\n            # ëŒ€í™”ê¸°ë¡ìš© key ì¸ chat_history ëŠ” ê°€ê¸‰ì  ë³€ê²½ ì—†ì´ ì‚¬ìš©í•˜ì„¸ìš”!\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\"human\", \"#Question:\\\\n{question}\"),  # ì‚¬ìš©ìž ìž…ë ¥ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©\\n        ]\\n    )\\n\\n    # llm ìƒì„±\\n    llm = ChatOpenAI(model_name=\"gpt-4o\")\\n\\n    # ì¼ë°˜ Chain ìƒì„±\\n    chain = prompt | llm | StrOutputParser()\\n\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\\n        input_messages_key=\"question\",  # ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì´ í…œí”Œë¦¿ ë³€ìˆ˜ì— ë“¤ì–´ê°ˆ key\\n        history_messages_key=\"chat_history\",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤\\n    )\\n    return chain_with_history\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­\\nwarning_msg = st.empty()\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = create_chain(model_name=selected_model)\\n\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n    if chain is not None:\\n        response = chain.stream(\\n            # ì§ˆë¬¸ ìž…ë ¥\\n            {\"question\": user_input},\\n            # ì„¸ì…˜ ID ê¸°ì¤€ìœ¼ë¡œ ëŒ€í™”ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n\\n        # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n        st.chat_message(\"user\").write(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n            # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n            add_message(\"user\", user_input)\\n            add_message(\"assistant\", ai_answer)\\n    else:\\n        # ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥\\n        warning_msg.error(\"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\02-Email\\\\main.py'}, page_content='import os\\nfrom dotenv import load_dotenv\\nimport streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_core.output_parsers import PydanticOutputParser\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_community.utilities import SerpAPIWrapper\\nfrom langchain_teddynote.prompts import load_prompt\\n\\n\\n# ê²€ìƒ‰ì„ ìœ„í•œ API KEY ì„¤ì •\\nos.environ[\"SERPAPI_API_KEY\"] = (\\n    \"e76de14ee240e0051ed8bb05d5db568dd1dc9cfcaa2b51fd83613829a85bf244\"\\n)\\n\\n\\n# ì´ë©”ì¼ ë³¸ë¬¸ìœ¼ë¡œë¶€í„° ì£¼ìš” ì—”í‹°í‹° ì¶”ì¶œ\\nclass EmailSummary(BaseModel):\\n    person: str = Field(description=\"ë©”ì¼ì„ ë³´ë‚¸ ì‚¬ëžŒ\")\\n    company: str = Field(description=\"ë©”ì¼ì„ ë³´ë‚¸ ì‚¬ëžŒì˜ íšŒì‚¬ ì •ë³´\")\\n    email: str = Field(description=\"ë©”ì¼ì„ ë³´ë‚¸ ì‚¬ëžŒì˜ ì´ë©”ì¼ ì£¼ì†Œ\")\\n    subject: str = Field(description=\"ë©”ì¼ ì œëª©\")\\n    summary: str = Field(description=\"ë©”ì¼ ë³¸ë¬¸ì„ ìš”ì•½í•œ í…ìŠ¤íŠ¸\")\\n    date: str = Field(description=\"ë©”ì¼ ë³¸ë¬¸ì— ì–¸ê¸‰ëœ ë¯¸íŒ… ë‚ ì§œì™€ ì‹œê°„\")\\n\\n\\n# API KEY ì •ë³´ë¡œë“œ\\nload_dotenv()\\n\\nst.title(\"Email ìš”ì•½ê¸° ðŸ’¬\")\\n\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ\\nif \"messages\" not in st.session_state:\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.\\n    st.session_state[\"messages\"] = []\\n\\n# ì‚¬ì´ë“œë°” ìƒì„±\\nwith st.sidebar:\\n    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_email_parsing_chain():\\n    # PydanticOutputParser ìƒì„±\\n    output_parser = PydanticOutputParser(pydantic_object=EmailSummary)\\n\\n    prompt = PromptTemplate.from_template(\\n        \"\"\"\\n    You are a helpful assistant. Please answer the following questions in KOREAN.\\n\\n    #QUESTION:\\n    ë‹¤ìŒì˜ ì´ë©”ì¼ ë‚´ìš© ì¤‘ì—ì„œ ì£¼ìš” ë‚´ìš©ì„ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.\\n\\n    #EMAIL CONVERSATION:\\n    {email_conversation}\\n\\n    #FORMAT:\\n    {format}\\n    \"\"\"\\n    )\\n\\n    # format ì— PydanticOutputParserì˜ ë¶€ë¶„ í¬ë§·íŒ…(partial) ì¶”ê°€\\n    prompt = prompt.partial(format=output_parser.get_format_instructions())\\n\\n    # ì²´ì¸ ìƒì„±\\n    chain = prompt | ChatOpenAI(model=\"gpt-4-turbo\") | output_parser\\n\\n    return chain\\n\\n\\ndef create_report_chain():\\n    prompt = load_prompt(\"prompts/email.yaml\", encoding=\"utf-8\")\\n\\n    # ì¶œë ¥ íŒŒì„œ\\n    output_parser = StrOutputParser()\\n\\n    # ì²´ì¸ ìƒì„±\\n    chain = prompt | ChatOpenAI(model=\"gpt-4-turbo\") | output_parser\\n\\n    return chain\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...\\nif user_input:\\n    # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n    st.chat_message(\"user\").write(user_input)\\n\\n    # 1) ì´ë©”ì¼ì„ íŒŒì‹±í•˜ëŠ” chain ì„ ìƒì„±\\n    email_chain = create_email_parsing_chain()\\n    # email ì—ì„œ ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì²´ì¸ì„ ì‹¤í–‰\\n    answer = email_chain.invoke({\"email_conversation\": user_input})\\n\\n    # 2) ë³´ë‚¸ ì‚¬ëžŒì˜ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘(ê²€ìƒ‰)\\n    params = {\"engine\": \"google\", \"gl\": \"kr\", \"hl\": \"ko\", \"num\": \"3\"}  # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°\\n    search = SerpAPIWrapper(params=params)  # ê²€ìƒ‰ ê°ì²´ ìƒì„±\\n    search_query = f\"{answer.person} {answer.company} {answer.email}\"  # ê²€ìƒ‰ ì¿¼ë¦¬\\n    search_result = search.run(search_query)  # ê²€ìƒ‰ ì‹¤í–‰\\n    search_result = eval(search_result)  # list í˜•íƒœë¡œ ë³€í™˜\\n\\n    # ê²€ìƒ‰ ê²°ê³¼(í•©ì¹˜ê¸°)\\n    search_result_string = \"\\\\n\".join(search_result)\\n\\n    # 3) ì´ë©”ì¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±\\n    report_chain = create_report_chain()\\n    report_chain_input = {\\n        \"sender\": answer.person,\\n        \"additional_information\": search_result_string,\\n        \"company\": answer.company,\\n        \"email\": answer.email,\\n        \"subject\": answer.subject,\\n        \"summary\": answer.summary,\\n        \"date\": answer.date,\\n    }\\n\\n    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n    response = report_chain.stream(report_chain_input)\\n    with st.chat_message(\"assistant\"):\\n        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\MyProject\\\\1.project\\\\main.py'}, page_content='import streamlit as st\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"ëŒ€í™”ì´ˆê¸°í™”\")\\n\\n    selected_prompt = st.selectbox(\\n        \"í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”\", (\"ê¸°ë³¸ëª¨ë“œ\", \"SNS ê²Œì‹œê¸€\", \"ìš”ì•½\")\\n    )\\n\\n    selected_model = st.selectbox(\\n        \"modelì„ ì„ íƒí•´ ì£¼ì„¸ìš”\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\ndef print_messags():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\nfrom langchain_core.messages import ChatMessage\\n\\n\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain import hub\\n\\n\\n# ì²´ì¸ ìƒì„±\\ndef create_chain(prompt_type, model_name=\"gpt-4o-mini\"):\\n\\n    # prompt ê¸°ë³¸ íƒ€ìž…\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI assistantìž…ë‹ˆë‹¤. ë‹¤ìŒì˜ ëŒ€ë‹µì— ê°„ê²°í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”.\",\\n            ),\\n            (\"user\", \"#Question: \\\\n{question}\"),\\n        ]\\n    )\\n\\n    if prompt_type == \"SNS ê²Œì‹œê¸€\":\\n        prompt = load_prompt(\"prompts/sns.yaml\", encoding=\"utf-8\")\\n\\n    elif prompt_type == \"ìš”ì•½\":\\n        prompt = load_prompt(\"prompts/summary2.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name)\\n\\n    chain = prompt | llm | StrOutputParser()\\n    return chain\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messags()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”\")\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´\\n\\nif user_input:\\n    # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n    st.chat_message(\"user\").write(user_input)\\n    # ì²´ì¸ ìƒì„±\\n    chain = create_chain(selected_prompt, selected_model)\\n\\n    response = chain.stream({\"question\": user_input})\\n\\n    with st.chat_message(\"assistant\"):\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # ëŒ€í™” ê¸°ë¡ì„ ì €ìž¥\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n\\n# streamlit run\\n'),\n",
       " Document(metadata={'source': '..\\\\MyProject\\\\1.project\\\\pages\\\\image_rag.py'}, page_content=''),\n",
       " Document(metadata={'source': '..\\\\MyProject\\\\1.project\\\\pages\\\\pdf_rag.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nload_dotenv()\\n\\n\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\nst.title(\"ìž¬í˜¸ì˜ RAG\")\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"ì´ˆê¸°í™”ë²„íŠ¼\")\\n\\n    uploaded_file = st.file_uploader(\"íŒŒì¼ì„ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    selected_model = st.selectbox(\\n        \"ëª¨ë¸ì„ íƒ\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n@st.cache_resource(show_spinner=\"ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\".cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n\\n    return retriever\\n\\n\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    prompt = load_prompt(\"../prompts/pdf-rag.yaml\")\\n\\n    llm = ChatOpenAI(model=model_name)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nprint_messages()\\n\\nuser_input = st.chat_input(\"ë©”ì„¸ì§€ë¥¼ ìž…ë ¥í•´ì£¼ì„¸ìš”\")\\n\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        st.chat_message(\"user\").write(user_input)\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\MyProject\\\\7ì›” ë¼ì´ë¸Œ\\\\CustomLoader.py'}, page_content='from typing import Iterator\\nfrom typing import Any, Dict, List, Optional\\nfrom langchain_core.document_loaders import BaseLoader\\nfrom langchain_core.documents import Document\\n\\nclass CustomDocumentLoader(BaseLoader):\\n    def __init__(self, file_path: str) - > None:\\n        \"\"\" ë¡œë”ë¥¼ íŒŒì¼ ê²½ë¡œì™€ í•¨ê»˜ ì´ˆê¸°í™” í•œë‹¤.\\n        \\n        Args:\\n            file_path: ë¡œë“œí•  íŒŒì¼ì˜ ê²½ë¡œ\\n        \"\"\"\\n\\n        self.file_path = file_path\\n\\n    def lazy_load(self) -> Iterator[Document]:\\n        \\n    \\n        with open'),\n",
       " Document(metadata={'source': '..\\\\MyProject\\\\7ì›” ë¼ì´ë¸Œ\\\\LlamaParseLoader.py'}, page_content='import os\\nfrom typing import Iterator\\nfrom typing import Any, Dict, List, Optional\\nfrom langchain_core.document_loaders import BaseLoader\\nfrom langchain_core.documents import Document\\nfrom llama_parse import LlamaParse\\nfrom llama_index.core import SimpleDirectoryReader\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n\\nclass LlamaParseLoader(BaseLoader):\\n    \"\"\"íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ì˜¤ëŠ” ë¬¸ì„œ ë¡œë”ì˜ ì˜ˆì‹œìž…ë‹ˆë‹¤.\"\"\"\\n\\n    def __init__(self, file_paths: List[str], parsing_instructions=\"\") -> None:\\n        \"\"\"ë¡œë”ë¥¼ íŒŒì¼ ê²½ë¡œì™€ í•¨ê»˜ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\\n        Args:\\n            file_paths: ë¡œë“œí•  íŒŒì¼ì˜ ê²½ë¡œìž…ë‹ˆë‹¤.\\n        \"\"\"\\n        # LlamaParse ì„¤ì •\\n        parser = LlamaParse(\\n            # api_key=\"llx-...\",  # API í‚¤ (í™˜ê²½ ë³€ìˆ˜ LLAMA_CLOUD_API_KEYì— ì €ìž¥ ê°€ëŠ¥)\\n            result_type=\"markdown\",  # ê²°ê³¼ íƒ€ìž…: \"markdown\" ë˜ëŠ” \"text\"\\n            num_workers=4,  # ì—¬ëŸ¬ íŒŒì¼ ì²˜ë¦¬ ì‹œ API í˜¸ì¶œ ë¶„í•  ìˆ˜\\n            verbose=True,\\n            language=\"ko\",  # ì–¸ì–´ ì„¤ì • (ê¸°ë³¸ê°’: \\'en\\')\\n            invalidate_cache=True,\\n            skip_diagonal_text=True,\\n            use_vendor_multimodal_model=True,\\n            vendor_multimodal_model_name=\"openai-gpt4o\",\\n            vendor_multimodal_api_key=os.environ.get(\"OPENAI_API_KEY\"),\\n            parsing_instruction=parsing_instructions,\\n        )\\n\\n        file_extractor = {\".pdf\": parser}\\n\\n        self.document_reader = SimpleDirectoryReader(\\n            input_files=file_paths,\\n            file_extractor=file_extractor,\\n        )\\n\\n    def lazy_load(self) -> Iterator[Document]:  # <-- ì¸ìžë¥¼ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤\\n        \"\"\"íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ì˜¤ëŠ” ì§€ì—° ë¡œë”ìž…ë‹ˆë‹¤.\\n\\n        ì§€ì—° ë¡œë“œ ë©”ì†Œë“œë¥¼ êµ¬í˜„í•  ë•ŒëŠ”, ë¬¸ì„œë¥¼ í•˜ë‚˜ì”© ìƒì„±í•˜ì—¬ ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\\n        \"\"\"\\n        documents = self.document_reader.load_data()\\n        langchain_documents = [doc.to_langchain_format() for doc in documents]\\n        return langchain_documents\\n\\n\\n# <ë”œë ˆë§ˆ>\\n# ë©€í‹°ëª¨ë‹¬ì´ ê°œìž…ë˜ë©´ hallucination ì´ í•„ì—°ì ìœ¼ë¡œ ë°œìƒ\\n# 100% ê²€ìˆ˜ë¥¼ í•˜ë ¤ë©´ ì¶”ê°€ í† í°ì´ í•„ìš”\\n# ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ê¸°ì—…ì´ ë¹„ìš© ê°ë‚´ X\\n'),\n",
       " Document(metadata={'source': '..\\\\ê³¼ì œ\\\\02.free_pdf_rag.py'}, page_content='# - PDF íŒŒì¼ ì—…ë¡œë“œ\\n# - FAISS ë²¡í„°ìŠ¤í† ì–´ì— ë°ì´í„° ì €ìž¥\\n# - ì—…ë¡œë“œí•œ íŒŒì¼ ê¸°ë°˜ Q&A ì±—ë´‡\\n# - ìœ ë£Œ ëª¨ë¸ ì‚¬ìš©ê¸ˆì§€!!\\n\\n############################\\nimport os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_experimental.text_splitter import SemanticChunker\\nfrom langchain_huggingface import HuggingFaceEndpointEmbeddings\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nload_dotenv()\\n\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\nst.title(\"PDFê¸°ë°˜ RAG\")\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"ì´ˆê¸°í™”ë²„íŠ¼\")\\n\\n    uploaded_file = st.file_uploader(\"PDFíŒŒì¼ ì—…ë¡œë“œ\", type=[\"pdf\"])\\n\\n    selected_model = st.selectbox(\"ëª¨ë¸ì„ íƒ\", [\"llama3.1\"])\\n\\n\\n@st.cache_resource(show_spinner=\"íŒŒì¼ì„ ì—…ë¡œë“œì¤‘ ìž…ë‹ˆë‹¤.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------ indexing\\n    # Text Split\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # Embedding Model\\n    embeddings = HuggingFaceEndpointEmbeddings(\\n        model=\"BAAI/bge-m3\", task=\"feature-extraction\"\\n    )\\n\\n    # Document loader\\n    text_splitter = SemanticChunker(embeddings)\\n    documents = text_splitter.split_documents(docs)\\n    # text_splitter.split_documents(file)\\n\\n    # Vector Store\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\ndef create_chain(retriever, model_name=\"llama3.1\"):\\n    prompt = load_prompt(\"prompts/pdf-rag-ollama.yaml\", encoding=\"utf-8\")\\n    # llm\\n    llm = ChatOllama(model=model_name)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nprint_messages()\\n\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”\")\\n\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        st.chat_message(\"user\").write(user_input)\\n\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”\")\\n'),\n",
       " Document(metadata={'source': '..\\\\ê¼­ ë³µìŠµ\\\\streamlit.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_community.document_loaders import PyPDFDirectoryLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.runnables import RunnablePassthrough, RunnableParallel\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain import hub\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\nst.title(\"Pratice streamlit ChatGPTðŸ’¬\")\\n\\n\\n# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ, st.session_state: ë³€ìˆ˜ë¥¼ ì €ìž¥í•˜ê¸° ìœ„í•¨.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# ì‚¬ì´ë“œë°”\\nwith st.sidebar:\\n    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼\\n    clear_btn = st.button(\"ëŒ€í™” ì´ˆê¸°í™”\")\\n\\n    # ì…€ë ‰íŠ¸ë°•ìŠ¤\\n    selected_prompt = st.selectbox(\\n        \"í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”\", (\"ê¸°ë³¸ëª¨ë“œ\", \"SNS ê²Œì‹œê¸€\", \"ìš”ì•½\"), index=0\\n    )\\n\\n\\n# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(\\n            chat_message.content\\n        )  # ë¡¤ì„ ìž‘ì„±í•˜ê³  ë‚´ìš©ì„ ìž‘ì„±.\\n\\n\\n# ìƒˆë¡œìš´ ë©”ì„¸ì§€ë¥¼ ì¶”ê°€\\n# from langchain_core.messages.chat import ChatMessage\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\ndef create_chain(prompt_type):\\n\\n    # í”„ë¡¬í”„íŠ¸-ê¸°ë³¸ëª¨ë“œ\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ìž…ë‹ˆë‹¤. ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\",\\n            ),\\n            (\\n                \"user\",\\n                \"\"\"\\n                \"#Context: \\\\n {context}\"\\n                \"#Question: \\\\n {question}\"\\n                \"\"\",\\n            ),\\n        ]\\n    )\\n\\n    if prompt_type == \"SNS ê²Œì‹œê¸€\":\\n        prompt = load_prompt(\"prompts/sns.yaml\", encoding=\"utf-8\")\\n\\n    elif prompt_type == \"ìš”ì•½\":\\n        prompt = hub.pull(\"teddynote/chain-of-density-korean:946ed62d\")\\n\\n    # Load data\\n    docs = PyPDFDirectoryLoader(\"data/RAG\")\\n    docs = docs.load()\\n\\n    # Split data\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # Embedding\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # Vector index\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n\\n    # Model\\n    llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\\n\\n    # ì¶œë ¥íŒŒì„œ\\n    output_parser = StrOutputParser()\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | output_parser\\n    )\\n    return chain\\n\\n\\n# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥\\nprint_messages()\\n\\n# ì‚¬ìš©ìžì˜ ìž…ë ¥\\nuser_input = st.chat_input(\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\")\\n\\n# ë§Œì•½, ì‚¬ìš©ìžì˜ ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´\\nif user_input:\\n    # ì‚¬ìš©ìžì˜ ìž…ë ¥\\n    st.chat_message(\"user\").write(user_input)\\n    # chainì„ ìƒì„±\\n    chain = create_chain(selected_prompt)\\n\\n    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ\\n    # response = chain.stream({\"question\": user_input})\\n    response = chain.stream(user_input)\\n    with st.chat_message(\"assistant\"):\\n\\n        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆë¥¼) ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìƒì„±í•œë‹¤.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # ëŒ€í™” ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n\\n\\n# https://github.com/teddylee777/langchain-kr/blob/main/19-Streamlit/01-MyProject/main.py\\n# streamlit run 19-Streamlit\\\\MyProject\\\\main.py\\n'),\n",
       " Document(metadata={'source': '..\\\\ë³„ë„\\\\2.Streamlitì„ í™œìš©í•œ í”„ë¡œì íŠ¸\\\\app.py'}, page_content='import streamlit as st\\nimport yfinance as yf\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\n\\nfrom yfinance_data import ìž¬ë¬´ì œí‘œì²˜ë¦¬, ìž¬ë¬´ì œí‘œì‹œê°í™”, ê°€ì¹˜ì£¼, ìš°ëŸ‰ì£¼, ê±°ëž˜ëŸ‰\\nfrom backend import AI_report\\nfrom meilisearch_search import search_stocks\\n\\n\\n# ì‹¬ë³¼ê³¼ ë„¤ê¹€ ê°€ì ¸ì˜¤ê¸°\\nclass SearchResults:\\n    def __init__(self, item):\\n        self.item = item\\n\\n    def __getitem__(self, key):\\n        return self.item[key]\\n\\n    def __str__(self):\\n        return f\\'{self.item[\"Symbol\"]} - {self.item[\"Name\"]}\\'\\n\\n\\n# ì£¼ì‹ í‹°ì»¤\\nticker = \"AAPL\"\\n\\nstock = yf.Ticker(ticker)\\n\\nst.title(\"ì£¼ì‹ ì •ë³´ ëŒ€ì‹œë³´ë“œ\")\\n\\nsearch_query = st.text_input(\"ê²€ìƒ‰ì°½\")\\nhits = search_stocks(search_query)[\"hits\"]\\nsearch_results = [SearchResults(hit) for hit in hits]\\n\\nselected = st.selectbox(\"ê²€ìƒ‰ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\", search_results)\\n\\ntabs = [\"íšŒì‚¬ ê¸°ë³¸ ì •ë³´\", \"AI ë¶„ì„ ë³´ê³ ì„œ\", \"ì¢…ëª© í† ë¡ ì‹¤\"]\\ntab1, tab2, tab3 = st.tabs(tabs)\\n\\nwith tab1:\\n    st.write(f\"ì„ íƒëœ í•­ëª©:{selected[\\'Symbol\\']}\")\\n    ìž¬ë¬´ì œí‘œë°ì´í„° = ìž¬ë¬´ì œí‘œì²˜ë¦¬()\\n    st.header(\"íšŒì‚¬ ê¸°ë³¸ ì •ë³´\")\\n\\n    # Balance Sheet ì‹œê°í™”\\n    st.subheader(f\"{ticker} - Balance Sheet\")\\n    balance_sheet_df = pd.DataFrame([ìž¬ë¬´ì œí‘œë°ì´í„°[\"Balance Sheet\"]])\\n    ax = balance_sheet_df.plot(kind=\"bar\", figsize=(10, 6))\\n    plt.title(f\"{ticker} - Balance Sheet\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    st.pyplot(fig=ax.figure)\\n\\n    # Income Statement ì‹œê°í™”\\n    st.subheader(f\"{ticker} - Income Statement\")\\n    income_statement_df = pd.DataFrame([ìž¬ë¬´ì œí‘œë°ì´í„°[\"Income Statement\"]])\\n    ax = income_statement_df.plot(kind=\"bar\", figsize=(10, 6))\\n    plt.title(f\"{ticker} - Income Statement\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    st.pyplot(fig=ax.figure)\\n\\n    # Cash Flow Statement ì‹œê°í™”\\n    st.subheader(f\"{ticker} - Cash Flow Statement\")\\n    cash_flow_df = pd.DataFrame([ìž¬ë¬´ì œí‘œë°ì´í„°[\"Cash Flow Statement\"]])\\n    ax = cash_flow_df.plot(kind=\"bar\", figsize=(10, 6))\\n    plt.title(f\"{ticker} - Cash Flow Statement\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    st.pyplot(fig=ax.figure)\\n\\n    # ê°€ì¹˜ì£¼ ì¶œë ¥##################################################################################################\\n    st.subheader(f\"Value Metrics\")\\n    # ê°€ì¹˜ì£¼() í•¨ìˆ˜ ì‹¤í–‰\\n    data = ê°€ì¹˜ì£¼()\\n\\n    # ì§€í‘œ ê°’ ì¶”ì¶œ\\n    pe_ratio = data[\"pe_ratio\"]\\n    pb_ratio = data[\"pb_ratio\"]\\n    peg_ratio = data[\"peg_ratio\"]\\n    dividend_yield = data[\"dividend_yield\"]\\n    ev_ebitda_ratio = data[\"ev_ebitda_ratio\"]\\n\\n    # ì‹œê°í™”í•  ê°’ê³¼ ë ˆì´ë¸” ì„¤ì •\\n    labels = [\"P/E Ratio\", \"P/B Ratio\", \"PEG Ratio\", \"Dividend Yield\", \"EV/EBITDA\"]\\n    values = [pe_ratio, pb_ratio, peg_ratio, dividend_yield, ev_ebitda_ratio]\\n\\n    # Matplotlib ë§‰ëŒ€ ê·¸ëž˜í”„ ìƒì„±\\n    fig, ax = plt.subplots()\\n    ax.bar(labels, values, color=[\"blue\", \"green\", \"red\", \"purple\", \"orange\"])\\n    ax.set_title(\"Stock Valuation Ratios\")\\n    ax.set_ylabel(\"Ratio Value\")\\n    ax.set_xticklabels(labels, rotation=45)\\n\\n    # ê·¸ëž˜í”„ë¥¼ Streamlitì— í‘œì‹œ\\n    st.pyplot(fig)\\n\\n    # ë°ì´í„° í”„ë ˆìž„ìœ¼ë¡œ ìƒì„±\\n    # ë°ì´í„° í”„ë ˆìž„ ìƒì„±\\n    df = pd.DataFrame(data.items(), columns=[\"Metric\", \"Value\"])\\n\\n    # Streamlitì— ë°ì´í„° í”„ë ˆìž„ í‘œì‹œ\\n    st.subheader(\"ê°€ì¹˜ì£¼ ì§€í‘œ ë°ì´í„° í”„ë ˆìž„\")\\n    st.dataframe(df)\\n\\n    # # ë°ì´í„° í”„ë ˆìž„ì„ ì´ìš©í•œ ë§‰ëŒ€ ì°¨íŠ¸\\n    # st.subheader(\"ê°€ì¹˜ì£¼ ì§€í‘œ ë§‰ëŒ€ ì°¨íŠ¸\")\\n    # st.bar_chart(df.set_index(\"Metric\"))\\n\\n    # ìš°ëŸ‰ì£¼ ì¶œë ¥ ##################################################################################################\\n    st.title(\"ìš°ëŸ‰ì£¼ ì§€í‘œ ì‹œê°í™”\")\\n\\n    # ìš°ëŸ‰ì£¼() í•¨ìˆ˜ ì‹¤í–‰\\n    data2 = ìš°ëŸ‰ì£¼()\\n\\n    # ë°ì´í„° í”„ë ˆìž„ ìƒì„±\\n    df = pd.DataFrame(data2.items(), columns=[\"Metric\", \"Value\"])\\n\\n    # Streamlitì— ë°ì´í„° í”„ë ˆìž„ í‘œì‹œ\\n    st.subheader(\"ìš°ëŸ‰ì£¼ ì§€í‘œ ë°ì´í„° í”„ë ˆìž„\")\\n    st.dataframe(df)\\n\\n    # # ë°ì´í„° í”„ë ˆìž„ì„ ì´ìš©í•œ ë§‰ëŒ€ ì°¨íŠ¸\\n    # st.subheader(\"ìš°ëŸ‰ì£¼ ì§€í‘œ ë§‰ëŒ€ ì°¨íŠ¸\")\\n    # st.bar_chart(df.set_index(\"Metric\"))\\n\\n    # ê±°ëž˜ëŸ‰ ì¶œë ¥ ##################################################################################################\\n    st.title(\"ê±°ëž˜ëŸ‰ ì§€í‘œ ì‹œê°í™”\")\\n    # ê±°ëž˜ëŸ‰ ë°ì´í„°ë§Œ ì¶”ì¶œ\\n    st.subheader(f\"{ticker} ì£¼ì‹ ê°€ê²©\")\\n    data3 = ê±°ëž˜ëŸ‰()\\n\\n    # Matplotlibë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™”\\n    fig, ax = plt.subplots(figsize=(10, 6))\\n\\n    # ë‚ ì§œë³„ë¡œ Open, High, Low, Close ë°ì´í„° í‘œì‹œ\\n    ax.plot(data3[\"volume\"].index, data3[\"open\"], label=\"Open\", color=\"blue\")\\n    ax.plot(data3[\"volume\"].index, data3[\"high\"], label=\"High\", color=\"green\")\\n    ax.plot(data3[\"volume\"].index, data3[\"low\"], label=\"Low\", color=\"red\")\\n    ax.plot(data3[\"volume\"].index, data3[\"close\"], label=\"Close\", color=\"purple\")\\n\\n    ax.set_xlabel(\"Date\")\\n    ax.set_ylabel(\"Price (USD)\")\\n    ax.set_title(f\"{ticker} stock price,(latest 1month)\")\\n    ax.legend()\\n\\n    # Streamlitì— ê·¸ëž˜í”„ ì¶œë ¥\\n    st.pyplot(fig)\\n\\n    # ê±°ëž˜ëŸ‰ ë°ì´í„°ë„ ë³„ë„ë¡œ í‘œì‹œ\\n    st.subheader(f\"{ticker} ì£¼ì‹ ê±°ëž˜ëŸ‰ ë°ì´í„°\")\\n    st.line_chart(data3[\"volume\"])\\n\\n\\nwith tab2:\\n    st.header(\"AIë¶„ì„ ë³´ê³ ì„œ\")\\n    if st.button(\"ë³´ê³ ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\"):\\n        with st.spinner(text=\"In progress\"):\\n            st.write(AI_report())\\n        st.success(\"Done\")\\n    # st.write(\"ì—¬ê¸°ì— AIë¶„ì„ ë³´ê³ ì„œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.\")\\n\\nwith tab3:\\n    st.header(\"ì¢…ëª© í† ë¡ ì‹¤\")\\n    st.write(\"ì—¬ê¸°ì— ì¢…ëª© í† ë¡ ì‹¤ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”.\")\\n\\n# yfinance python ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì£¼ìš” í•­ëª©ì— ë°ì´í„° ì‹œê°í™”ë¥¼ í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”\\n'),\n",
       " Document(metadata={'source': '..\\\\ë³„ë„\\\\2.Streamlitì„ í™œìš©í•œ í”„ë¡œì íŠ¸\\\\app2.py'}, page_content='import streamlit as st\\nimport sqlite3\\n\\nfrom stock_info import stock\\nfrom backend import AI_report\\nfrom meilisearch_search import search_stocks\\nfrom comments import create_connection, create_table, insert_comment, get_all_comments\\n\\n# from comment import create_connection, create_table, insert_comment, get_all_comments\\n\\n\\n# ìºì‹œë¥¼ ì´ìš©í•œë‹¤.\\n@st.cache_data\\ndef cache_AI_report(ticker):\\n    return AI_report(ticker)\\n\\n\\nclass SearchResult:\\n    def __init__(self, item):\\n        self.item = item\\n\\n    @property\\n    def symbol(self):\\n        return self.item[\"Symbol\"]\\n\\n    @property\\n    def name(self):\\n        return self.item[\"Name\"]\\n\\n    def __str__(self):\\n        return f\"{self.symbol}: {self.name}\"\\n\\n\\n# Set the page title\\nst.title(\"ì£¼ì‹ ì •ë³´ ë¶„ì„ ëŒ€ì‹œë³´ë“œ\")\\n\\n# Create a text input for search\\nsearch_query = st.text_input(\"ê²€ìƒ‰ì°½\")\\nhits = search_stocks(search_query)[\"hits\"]\\nsearch_results = [SearchResult(hit) for hit in hits]\\n\\n# Create a select box for search results list\\n\\nselected = st.selectbox(\"ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\", search_results)\\n\\n# Create tabs for different sections\\ntabs = [\"íšŒì‚¬ ê¸°ë³¸ ì •ë³´\", \"AI ë¶„ì„ ë³´ê³ ì„œ\", \"ì¢…ëª© í† ë¡ ì‹¤\"]\\ntab1, tab2, tab3 = st.tabs(tabs)\\n\\n# Content for \"íšŒì‚¬ ê¸°ë³¸ ì •ë³´\" tab\\nwith tab1:\\n    stock = stock(selected.symbol)\\n    st.header(str(selected))\\n    # ê±°ëž˜ëŸ‰ ì‹œê°í™”\\n    st.subheader(f\"ê±°ëž˜ëŸ‰\")\\n    stock_data = stock.ê¸ˆìœµì •ë³´()\\n    st.line_chart(stock_data[\"history\"][\"Volume\"])\\n\\n    st.header(\"ìž¬ë¬´ì œí‘œ\")\\n    cols = st.columns(3)\\n    cols[0].subheader(\"ë§¤ì¶œì•¡\")\\n    cols[0].line_chart(stock_data[\"income_statement\"].loc[\"Total Revenue\"])\\n    cols[1].subheader(\"ìˆœì´ìµ\")\\n    cols[1].line_chart(stock_data[\"income_statement\"].loc[\"Net Income\"])\\n    cols[2].subheader(\"ì˜ì—…ì´ìµ\")\\n    cols[2].line_chart(stock_data[\"income_statement\"].loc[\"Operating Income\"])\\n\\n    cols = st.columns(3)\\n    cols[0].subheader(\"ìžì‚°\")\\n    cols[0].line_chart(stock_data[\"balance_sheet\"].loc[\"Total Assets\"])\\n    cols[1].subheader(\"ë¶€ì±„\")\\n    cols[1].line_chart(\\n        stock_data[\"balance_sheet\"].loc[\"Total Liabilities Net Minority Interest\"]\\n    )\\n    cols[2].subheader(\"ìžë³¸\")\\n    cols[2].line_chart(stock_data[\"balance_sheet\"].loc[\"Stockholders Equity\"])\\n\\n    cols = st.columns(4)\\n    cols[0].subheader(\"ì˜ì—… í˜„ê¸ˆíë¦„\")\\n    cols[0].line_chart(stock_data[\"cash_flow\"].loc[\"Operating Cash Flow\"])\\n    cols[1].subheader(\"íˆ¬ìž í˜„ê¸ˆíë¦„\")\\n    cols[1].line_chart(stock_data[\"cash_flow\"].loc[\"Investing Cash Flow\"])\\n    cols[2].subheader(\"ìž¬ë¬´ í˜„ê¸ˆíë¦„\")\\n    cols[2].line_chart(stock_data[\"cash_flow\"].loc[\"Financing Cash Flow\"])\\n    cols[3].subheader(\"ìˆœ í˜„ê¸ˆíë¦„\")\\n    cols[3].line_chart(stock_data[\"cash_flow\"].loc[\"Free Cash Flow\"])\\n\\n# Content for \"AI ë¶„ì„ ë³´ê³ ì„œ\" tab\\nwith tab2:\\n    st.header(\"AI ë¶„ì„ ë³´ê³ ì„œ\")\\n    if st.button(\"ë³´ê³ ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\"):\\n        with st.spinner(text=\"In progress\"):\\n            data = cache_AI_report(selected.symbol)\\n            st.success(\"Done\")\\n        st.write(data)\\n\\n# Content for \"ì¢…ëª© í† ë¡ ì‹¤\" tab\\nwith tab3:\\n    st.header(\"ì¢…ëª© í† ë¡ ì‹¤\")\\n    conn = create_connection()\\n    create_table(conn)\\n\\n    for comment in get_all_comments(conn):\\n        comment_time, comment_text = comment\\n        st.write(f\"{comment_time}: {comment_text}\")\\n\\n    # ì•žì—ì„œë¶€í„° ê·¸ë¦¬ê¸° ë•Œë¬¸ì— ëŒ“ê¸€ ìž…ë ¥ì°½ì´ ìœ„ì— ë‚˜ì˜´\\n    new_comment = st.text_area(\"ëŒ“ê¸€ì„ ìž…ë ¥í•˜ì„¸ìš”\")\\n    if st.button(\"ëŒ“ê¸€ ìž‘ì„±\"):\\n        insert_comment(conn, f\"{selected.name} - {new_comment}\")\\n        st.success(\"ëŒ“ê¸€ì´ ìž‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤\")\\n        st.rerun()\\n'),\n",
       " Document(metadata={'source': '..\\\\ë³„ë„\\\\2.Streamlitì„ í™œìš©í•œ í”„ë¡œì íŠ¸\\\\backend.py'}, page_content='from dotenv import load_dotenv\\n\\nload_dotenv()\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\n\\n# from yfinance_data import ìž¬ë¬´ì œí‘œ, ê°€ì¹˜ì£¼, ìš°ëŸ‰ì£¼, ê±°ëž˜ëŸ‰\\nfrom stock_info import stock\\n\\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\n\\ndef AI_report(ticker):\\n    \"\"\"\\n    ë¶„ì„ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•´ë“œë¦½ë‹ˆë‹¤.\\n    ì„¸ ê°œì˜ ë”°ì˜´í‘œê°€ í¬í•¨ëœ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œê°€ ì œê³µë©ë‹ˆë‹¤.\\n    ìž¬ë¬´ ë¶„ì„ê°€ë¡œì„œ ë³´ê³ ì„œì— ë‹´ê¸´ ìˆ˜ì¹˜ë¥¼ ìžì„¸ížˆ ì‚´íŽ´ë³´ê³ \\n    íšŒì‚¬ì˜ ì„±ìž¥ ì¶”ì„¸ì™€ ìž¬ë¬´ ì•ˆì •ì„±ì„ í‰ê°€í•˜ì—¬ ì‚¬ìš©ìžë“¤ì´ ìžìœ ë¡­ê²Œ í† ë¡ í•  ìˆ˜ ìžˆë„ë¡ ë•ìŠµë‹ˆë‹¤.\\n    ì‚¬ëžŒë“¤ì´ ê³µê°œ í† ë¡ ì„ í•  ìˆ˜ ìžˆë„ë¡ ê·€í•˜ì˜ ì˜ê²¬ì„ ì œê³µí•˜ì‹­ì‹œì˜¤.\\n    ë³´ê³ ì„œë¥¼ í•œêµ­ì–´ë¡œ ì œì¶œí•´ì£¼ì„¸ìš”.\\n    \"\"\"\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"\"\"\\n        I want you to act as a Financial Analyst.\\n        Want assistance provided by qualified individuals enabled with experience on understanding charts using technical analysis tools while interpreting macroeconomic environment prevailing across world consequently assisting customers acquire long term advantages requires clear verdicts therefore seeking same through informed predictions written down precisely! First statement contains following content- â€œCan you tell us what future stock market looks like based upon current conditions ?â€.\\n        \"\"\",\\n            ),\\n            (\\n                \"user\",\\n                \\'\\'\\' \\n        We provide the information necessary for analysis.\\n        Given markdown reports with triple quotes. \\n        As a Financial Analyst, Take a closer look at the numbers in the report and evaluate the company\\'s growth trends and financial stability to help users discuss freely.\\n        Provide your opinion to people so they can have an open discussion.\\n        Please provide the report in Korean.\\n\\n        \"\"\"\\n        {markdown}\\n        \"\"\"\\n        \\'\\'\\',\\n            ),\\n        ]\\n    )\\n    output_parser = StrOutputParser()\\n\\n    chain = prompt | llm | output_parser\\n    return chain.invoke({\"markdown\": stock(ticker).report_support()})\\n\\n\\n# def AI_report():\\n#     prompt = ChatPromptTemplate.from_messages(\\n#         [\\n#             (\\n#                 \"system\",\\n#                 \"\"\"\\n#             I want you to act as a Fianancial Analyst.\\n#             Want assistance provided by qualified individuals enabled with experience on understanding charts using technical analysis tools while interpreting macroeconomic environment prevailing across world consequently assisting customers acquire long term advantages requires clear verdicts therefore seeking same through informed predictions written down precisely! First statement contains following content- â€œCan you tell us what future stock market looks like based upon current conditions ?\".\\n#             \"\"\",\\n#             ),\\n#             (\\n#                 \"user\",\\n#                 \"\"\"\\n#                 As a financial analysis expert, please carefully examine these tables and analyze them to enable a free discussion on evaluating the company\\'s growth or value.\\n#                 You Answer Korean.\\n#                 #Balance Sheet: {ìž¬ë¬´ì œí‘œ}\\n#                 #Value Stock: {ê°€ì¹˜ì£¼}\\n#                 #Blue Chip Stock: {ìš°ëŸ‰ì£¼}\\n#                 #Volume: {ê±°ëž˜ëŸ‰}\\n#                 #Question: {input}\\n#             \"\"\",\\n#             ),\\n#         ]\\n#     )\\n\\n#     chain = prompt | llm | StrOutputParser()\\n#     return chain.invoke(\\n#         {\\n#             \"input\": \"ì• í”Œ ì£¼ê°„ ì°¨íŠ¸ë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”.\",\\n#             \"ìž¬ë¬´ì œí‘œ\": ìž¬ë¬´ì œí‘œ(),\\n#             \"ê°€ì¹˜ì£¼\": ê°€ì¹˜ì£¼(),\\n#             \"ìš°ëŸ‰ì£¼\": ìš°ëŸ‰ì£¼(),\\n#             \"ê±°ëž˜ëŸ‰\": ê±°ëž˜ëŸ‰(),\\n#         },\\n#     )\\n'),\n",
       " Document(metadata={'source': '..\\\\ë³„ë„\\\\2.Streamlitì„ í™œìš©í•œ í”„ë¡œì íŠ¸\\\\comments.py'}, page_content='import sqlite3\\n\\n\"\"\"\\nstreamlit ìœ¼ë¡œ ê°„ë‹¨í•œ ìµëª… ëŒ“ê¸€ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\\nSQLite ë¥¼ ì‚¬ìš©í•´ì„œ ëŒ“ê¸€ì„ ë³´ê´€í•  ìˆ˜ ìžˆë„ë¡ ì˜ˆì‹œì½”ë“œë¥¼ ìž‘ì„±í•´ ì£¼ì„¸ìš”\\n\"\"\"\\n# import sqlite3\\n\\n\\n# SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜\\ndef create_connection():\\n    conn = sqlite3.connect(\"comments.db\")\\n    return conn\\n\\n\\n# ëŒ“ê¸€ í…Œì´ë¸” ìƒì„± í•¨ìˆ˜\\ndef create_table(conn):\\n    with conn:\\n        conn.execute(\\n            \"\"\"\\n            CREATE TABLE IF NOT EXISTS comments (\\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\n                comment TEXT NOT NULL\\n            )\\n        \"\"\"\\n        )\\n\\n\\n# ëŒ“ê¸€ ì‚½ìž… í•¨ìˆ˜\\ndef insert_comment(conn, comment):\\n    with conn:\\n        conn.execute(\"INSERT INTO comments (comment) VALUES (?)\", (comment,))\\n\\n\\n# ëª¨ë“  ëŒ“ê¸€ ì¡°íšŒ í•¨ìˆ˜\\ndef get_all_comments(conn):\\n    with conn:\\n        comments = conn.execute(\\n            \"SELECT created_at, comment FROM comments ORDER BY created_at DESC\"\\n        ).fetchall()\\n    yield from comments\\n'),\n",
       " Document(metadata={'source': '..\\\\ë³„ë„\\\\2.Streamlitì„ í™œìš©í•œ í”„ë¡œì íŠ¸\\\\meilisearch_search.py'}, page_content='# meilisearch-windows-amd64ì„ ì¼œì•¼í•¨\\n# meilisearch-windows-amd64ì„ ì¼œì•¼í•¨\\nimport meilisearch\\nimport json\\nimport pandas as pd\\n\\nclient = meilisearch.Client(\"http://localhost:7700\", \"aSampleMasterKey\")\\n\\n# json_file = open(\"movies.json\", encoding=\"utf-8\")\\n# movies = json.load(json_file)\\n# client.index(\"movies\").add_documents(movies)\\n\\n# client.index(\"movies\").search(\"botman\")\\n\\ndf = pd.read_csv(\"nasdaq_screener_1726112812897.csv\", na_filter=False)\\nd = df.to_dict(orient=\"records\")  # column í•˜ë‚˜ì— rowí•˜ë‚˜ cellí•˜ë‚˜\\n\\n# ì •ê·œì‹ replaceì™€ stripì„ í†µí•´ ë„ì–´ì“°ê¸° ì œê±°í•œê²ƒì„ id columnì— ì €ìž¥\\ndf[\"id\"] = df[\"Symbol\"].str.strip().replace(r\"[/^]\", \"_\", regex=True)\\n\\nclient.index(\"stocks\").add_documents(df.to_dict(orient=\"records\"), primary_key=\"id\")\\n\\n\\n# ì¸ë±ìŠ¤ê²€ìƒ‰ í•¨ìˆ˜\\ndef search_stocks(query):\\n    index = client.index(\"stocks\")\\n    res = index.search(query)\\n    return res\\n'),\n",
       " Document(metadata={'source': '..\\\\ë³„ë„\\\\2.Streamlitì„ í™œìš©í•œ í”„ë¡œì íŠ¸\\\\stock_info.py'}, page_content='import pandas as pd\\nimport yfinance as yf\\n\\n\\nclass stock:\\n    def __init__(self, ticker) -> None:\\n        # ì£¼ì‹ í‹°ì»¤ ì„¤ì •\\n        self.ticker = ticker\\n\\n        # í‹°ì»¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\\n        self.stock = yf.Ticker(self.ticker)\\n\\n    def ê¸ˆìœµì •ë³´(self):\\n        return {\\n            \"info\": self.stock.info,\\n            \"income_statement\": self.stock.quarterly_income_stmt,\\n            \"balance_sheet\": self.stock.balance_sheet,\\n            \"cash_flow\": self.stock.cash_flow,\\n            \"history\": self.stock.history(period=\"1mo\"),\\n        }\\n\\n    def report_support(self):\\n        # ê¸ˆìœµ ì „ë¬¸ê°€ì˜ ë¶„ì„ì„ ë³´ì¡°í•œ ì§€í‘œë“¤.\\n        # stock.infoì— ìžˆëŠ”ê²ƒë“¤ì„ ëª¨ë‘ DataFrameìœ¼ë¡œ ë°”ê¿¨ë‹¤.\\n\\n        # ìˆ«ìžë§Œ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜\\n        def is_float(x):\\n            try:\\n                float(x)\\n                return True\\n            except ValueError:\\n                return False\\n            except TypeError:\\n                return False\\n\\n        stock = self.stock\\n        info = pd.DataFrame.from_dict(stock.info, orient=\"index\", columns=[\"Value\"])\\n        info = info[info[\"Value\"].apply(is_float)]\\n\\n        return f\\'\\'\\'\\n        ### Financials\\n        {info.to_markdown()}\\n\\n        #### Quarterly Income Statement\\n        {stock.quarterly_income_stmt.loc[[\\'Total Revenue\\', \\'Gross Profit\\', \\'Operating Income\\', \\'Net Income\\']].to_markdown()}\"\"\"\\n\\n        #### Quarterly Balance Sheet\\n        {stock.quarterly_balance_sheet.loc[[\\'Total Assets\\', \\'Total Liabilities Net Minority Interest\\', \\'Stockholders Equity\\']].to_markdown()}\"\"\"\\n\\n        #### Quarterly Cash Flow\\n        {stock.quarterly_cash_flow.loc[[\\'Operating Cash Flow\\', \\'Investing Cash Flow\\', \\'Financing Cash Flow\\']].to_markdown()}\"\"\"\\n        \\'\\'\\'\\n'),\n",
       " Document(metadata={'source': '..\\\\ë³„ë„\\\\2.Streamlitì„ í™œìš©í•œ í”„ë¡œì íŠ¸\\\\yfinance_data.py'}, page_content='import yfinance as yf\\nimport pandas as pd\\n\\n\\n# ChatGPTì§ˆë¬¸: yfinanceë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìž¬ë¬´ìž¬í‘œë¥¼ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼í—¤ìš”\\n\\n\\n# ì£¼ì‹ í‹°ì»¤\\nticker = \"AAPL\"\\n\\n# í‹°ì»¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\\nstock = yf.Ticker(ticker)\\n\\n# ìž¬ë¬´ì œí‘œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\\nincome_statement = stock.financials\\nbalance_sheet = stock.balance_sheet\\ncash_flow = stock.cashflow\\n\\n\\n# ChatGPTì§ˆë¬¸: yfinance ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì €í‰ê°€ëœ ê°€ì¹˜ì£¼ë¥¼ ë³´ë ¤ë©´ ì–´ë–¤ ì§€í‘œë¥¼ ë´ì•¼ í•˜ë‚˜ìš”?\\ndef ìž¬ë¬´ì œí‘œ():\\n    income_statement = stock.financials\\n    balance_sheet = stock.balance_sheet\\n    cash_flow = stock.cashflow\\n    return {\\n        \"income_statement\": income_statement,\\n        \"balance_sheet\": balance_sheet,\\n        \"cash_flow\": cash_flow,\\n    }\\n\\n\\n# yfinance python ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì£¼ìš” í•­ëª©ì— ë°ì´í„° ì‹œê°í™”ë¥¼ í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”\\ndef ìž¬ë¬´ì œí‘œì²˜ë¦¬():\\n    # ìž¬ë¬´ì œí‘œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ ì •ì˜\\n    # 1. í‹°ì»¤ ì‹¬ë³¼ì„ ìž…ë ¥í•˜ê³  í•´ë‹¹ ì£¼ì‹ì˜ ìž¬ë¬´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´\\n\\n    # 2. ìž¬ë¬´ ìƒíƒœí‘œ (Balance Sheet)\\n    balance_sheet = stock.balance_sheet\\n    balance_sheet_data = {\\n        \"Total Assets\": balance_sheet.loc[\"Total Assets\"].iloc[0],  # ì´ ìžì‚°\\n        \"Total Liabilities Net Minority Interest\": balance_sheet.loc[\\n            \"Total Liabilities Net Minority Interest\"\\n        ].iloc[\\n            0\\n        ],  # ì´ ë¶€ì±„\\n        \"Stockholders Equity\": balance_sheet.loc[\"Stockholders Equity\"].iloc[0],  # ìžë³¸\\n    }\\n\\n    # 3. ì†ìµ ê³„ì‚°ì„œ (Income Statement)\\n    income_statement = stock.financials\\n    income_statement_data = {\\n        \"Total Revenue\": income_statement.loc[\"Total Revenue\"].iloc[0],  # ì´ ë§¤ì¶œ\\n        \"Net Income\": income_statement.loc[\"Net Income\"].iloc[0],  # ìˆœì´ìµ\\n        \"Operating Income\": income_statement.loc[\"Operating Income\"].iloc[\\n            0\\n        ],  # ì˜ì—…ì´ìµ\\n    }\\n\\n    # 4. í˜„ê¸ˆ íë¦„í‘œ (Cash Flow Statement)\\n    cashflow_statement = stock.cashflow\\n    cashflow_statement_data = {\\n        \"Operating Cash Flow\": cashflow_statement.loc[\\n            \"Cash Flow From Continuing Operating Activities\"\\n        ].iloc[\\n            0\\n        ],  # ì˜ì—…í™œë™ìœ¼ë¡œ ì¸í•œ í˜„ê¸ˆíë¦„\\n        \"Investing Cash Flow\": cashflow_statement.loc[\\n            \"Cash Flow From Continuing Investing Activities\"\\n        ].iloc[\\n            0\\n        ],  # íˆ¬ìží™œë™ìœ¼ë¡œ ì¸í•œ í˜„ê¸ˆíë¦„\\n        \"Free Cash Flow\": cashflow_statement.loc[\"Free Cash Flow\"].iloc[\\n            0\\n        ],  # ìžìœ  í˜„ê¸ˆíë¦„\\n    }\\n\\n    # 5. ì„¸ ê°€ì§€ ìž¬ë¬´ì œí‘œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¶ì–´ì„œ ë°˜í™˜\\n    financial_data = {\\n        \"Balance Sheet\": balance_sheet_data,\\n        \"Income Statement\": income_statement_data,\\n        \"Cash Flow Statement\": cashflow_statement_data,\\n    }\\n\\n    return financial_data\\n\\n\\nimport matplotlib.pyplot as plt\\n\\n\\ndef ìž¬ë¬´ì œí‘œì‹œê°í™”():\\n    # ìž¬ë¬´ì œí‘œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\\n    ìž¬ë¬´ì œí‘œë°ì´í„° = ìž¬ë¬´ì œí‘œì²˜ë¦¬()\\n\\n    # ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ë˜ë¯€ë¡œ DataFrameìœ¼ë¡œ ë³€í™˜\\n    balance_sheet = pd.DataFrame([ìž¬ë¬´ì œí‘œë°ì´í„°[\"Balance Sheet\"]])\\n    income_statement = pd.DataFrame([ìž¬ë¬´ì œí‘œë°ì´í„°[\"Income Statement\"]])\\n    cash_flow = pd.DataFrame([ìž¬ë¬´ì œí‘œë°ì´í„°[\"Cash Flow Statement\"]])\\n\\n    # 1. Balance Sheet ì‹œê°í™”\\n    plt.figure(figsize=(14, 7))\\n    balance_sheet.plot(kind=\"bar\")\\n    plt.title(f\"{ticker} - Balance Sheet\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    plt.show()\\n\\n    # 2. Income Statement ì‹œê°í™”\\n    plt.figure(figsize=(14, 7))\\n    income_statement.plot(kind=\"bar\")\\n    plt.title(f\"{ticker} - Income Statement\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    plt.show()\\n\\n    # 3. Cash Flow Statement ì‹œê°í™”\\n    plt.figure(figsize=(14, 7))\\n    cash_flow.plot(kind=\"bar\")\\n    plt.title(f\"{ticker} - Cash Flow Statement\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    plt.show()\\n\\n    # 3. Cash Flow Statement ì‹œê°í™” (ì˜ì—…í™œë™, íˆ¬ìží™œë™, ìžìœ í˜„ê¸ˆíë¦„)\\n    cash_flow_data = cash_flow.loc[\\n        [\\n            \"Total Cash From Operating Activities\",\\n            \"Total Cashflows From Investing Activities\",\\n            \"Free Cash Flow\",\\n        ]\\n    ].T\\n    cash_flow_data.plot(kind=\"bar\", figsize=(10, 6))\\n    plt.title(f\"{ticker} - Cash Flow Statement\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Date\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    plt.show()\\n\\n\\ndef ê°€ì¹˜ì£¼():\\n    # ê³¼ê±° P/E ë¹„ìœ¨\\n    pe_ratio = stock.info.get(\"forwardPE\")\\n    # P/B ë¹„ìœ¨\\n    pb_ratio = stock.info.get(\"priceToBook\")\\n    # PEG ë¹„ìœ¨\\n    peg_ratio = stock.info.get(\"pegRatio\")\\n    # ë°°ë‹¹ìˆ˜ìµë¥ \\n    dividend_yield = stock.info.get(\"dividendYield\")\\n    # EV/EBITDA ë¹„ìœ¨\\n    ev_ebitda_ratio = stock.info.get(\"enterpriseToEbitda\")\\n\\n    # forward_pe = stock.info.get(\\'forwardPE\\')  # ë¯¸ëž˜ P/E ë¹„ìœ¨\\n    # roe = stock.info.get(\\'returnOnEquity\\')  # ROE\\n    # current_ratio = stock.info.get(\\'currentRatio\\')  # ìœ ë™ì„± ë¹„ìœ¨\\n    # debt_to_equity = stock.info.get(\\'debtToEquity\\')  # ë¶€ì±„ë¹„ìœ¨\\n    return {\\n        \"pe_ratio\": pe_ratio,\\n        \"pb_ratio\": pb_ratio,\\n        \"peg_ratio\": peg_ratio,\\n        \"dividend_yield\": dividend_yield,\\n        \"ev_ebitda_ratio\": ev_ebitda_ratio,\\n    }\\n\\n\\n# ChatGPTì§ˆë¬¸: ìš°ëŸ‰ì£¼ë¥¼ ìž˜ ì°¾ìœ¼ë ¤ë©´ì–´ë–¤ ì§€í‘œë¥¼ ë´ì•¼í•˜ë‚˜ìš”?\\ndef ìš°ëŸ‰ì£¼():\\n    # ROE (ìžê¸°ìžë³¸ì´ìµë¥ )\\n    roe = stock.info.get(\"returnOnEquity\")\\n    # ROA (ìžì‚°ìˆ˜ìµë¥ ã„¹)\\n    roa = stock.info.get(\"returnOnAssets\")\\n    # ë¶€ì±„ë¹„ìœ¨\\n    debt_to_equity = stock.info.get(\"debtToEquity\")\\n    # ì´ìž ë° ì„¸ì „ ì´ìµ\\n    interest_coverage = stock.info.get(\"ebitda\")\\n    # ë§¤ì¶œ ì„±ìž¥ë¥ \\n    revenue_growth = stock.info.get(\"revenueGrowth\")\\n    # ë°°ë‹¹ ì§€ê¸‰ë¥ \\n    dividend_ratio = stock.info[\"payoutRatio\"]\\n    # EPS (ì£¼ë‹¹ìˆœì´ìµ)\\n    eps = stock.info.get(\"trailingEps\")\\n    # í˜„ê¸ˆíë¦„\\n    operating_cash_flow = stock.info[\"operatingCashflow\"]\\n\\n    return {\\n        \"roe\": roe,\\n        \"roa\": roa,\\n        \"debt_to_equity\": debt_to_equity,\\n        \"interest_coverage\": interest_coverage,\\n        \"revenue_growth\": revenue_growth,\\n        \"dividend_ratio\": dividend_ratio,\\n        \"eps\": eps,\\n        \"operating_cash_flow\": operating_cash_flow,\\n    }\\n\\n\\ndef ê±°ëž˜ëŸ‰():\\n    hist = stock.history(period=\"1mo\")\\n    return {\\n        \"volume\": hist[\"Volume\"],\\n        \"open\": hist[\"Open\"],\\n        \"high\": hist[\"High\"],\\n        \"low\": hist[\"Low\"],\\n        \"close\": hist[\"Close\"],\\n    }\\n'),\n",
       " Document(metadata={'source': '..\\\\ë³„ë„\\\\2.Streamlitì„ í™œìš©í•œ í”„ë¡œì íŠ¸2-wine_paring\\\\backend.py'}, page_content='from dotenv import load_dotenv\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\nsystem_template = \"\"\"\\nYou are ChatGPT, a professional sommelier who has gone through a rigorous training process step by step, driven by a deep curiosity about wine. You possess a keen sense of smell, a keen sense of exploration, and an awareness of the many details in wine.\\n\\nYour task is to accurately identify an appropriate wine pairing based on the provided food description in triple quotes.\\n\\nYou anser Korean.\\n\\n### Task Instructions:\\n\\n\\n**Pairing Recommendation**:\\n- Recommend a specific wine (including grape variety, region of origin, and possible vintage) that pairs well with the described food.\\n- Explain why this wine is a suitable match for the food, taking into account factors such as acidity, tannin structure, body, and flavor profile.\\n\\n### Example:\\n\\n**Review**\\n\\n\"\"\"\\n\\nprompt_template = ChatPromptTemplate.from_messages(\\n    [(\"system\", system_template), (\"user\", \"{text}\")]\\n)\\n\\n\\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\\nparser = StrOutputParser()\\n\\nchain = prompt_template | llm | parser\\nresponse = chain.invoke(\\n    {\\n        \"text\": \"ì™€ì¸ì€ ë ˆë“œ ì™€ì¸ìœ¼ë¡œ, í’ë¶€í•œ ê³¼ì¼í–¥ì´ ë‚˜ë©°, ë¶€ë“œë †ê³  ê¹Šì€ë§›ì´ ë‚œë‹¤. íƒ„ë‹Œì´ ì ë‹¹í•˜ê³  ì‚°ë„ê°€ ë†’ë‹¤.  ì–´ìš¸ë¦¬ëŠ” ìŒì‹ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.\"\\n    }\\n)\\n'),\n",
       " Document(metadata={'source': '..\\\\ë³„ë„\\\\2.Streamlitì„ í™œìš©í•œ í”„ë¡œì íŠ¸2-wine_paring\\\\openai_runnable.py'}, page_content='from openai import OpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nclient = OpenAI()\\n\\n\\ndef openai_runnable(query):\\n    response = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\\n                \"role\": \"system\",\\n                \"content\": [\\n                    {\\n                        \"type\": \"text\",\\n                        \"text\": \"You are ChatGPT, a professional sommelier who has gone through a rigorous training process step by step, driven by a deep curiosity about wine. You possess a keen sense of smell, a keen sense of exploration, and an awareness of the many details in wine.\\\\r\\\\n\\\\r\\\\nYour task is to accurately identify an appropriate wine pairing based on the provided food description in triple quotes.\\\\r\\\\n\\\\r\\\\nYou anser Korean.\\\\r\\\\n\\\\r\\\\n### Task Instructions:\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n**Pairing Recommendation**:\\\\r\\\\n- Recommend a specific wine (including grape variety, region of origin, and possible vintage) that pairs well with the described food.\\\\r\\\\n- Explain why this wine is a suitable match for the food, taking into account factors such as acidity, tannin structure, body, and flavor profile.\\\\r\\\\n\\\\r\\\\n### Example:\\\\r\\\\n\\\\r\\\\n**Review**\\\\r\\\\n\",\\n                    }\\n                ],\\n            },\\n            {\\n                \"role\": \"user\",\\n                \"content\": [\\n                    {\\n                        \"type\": \"image_url\",\\n                        \"image_url\": {\\n                            \"url\": \"https://images.vivino.com/thumbs/tiV02HEuQPaNoSRcWA3r2g_pb_x600.png\"\\n                        },\\n                    },\\n                    {\\n                        \"type\": \"text\",\\n                        \"text\": query,\\n                    },\\n                ],\\n            },\\n            {\\n                \"role\": \"assistant\",\\n                \"content\": [\\n                    {\\n                        \"type\": \"text\",\\n                        \"text\": \"**ìŒì‹ ì¶”ì²œ**: \\\\r\\\\nì•„ë§ˆë¡œë„¤ ë¸ë¼ ë°œí´ë¦¬ì²¼ë¼ ì™€ì¸ì€ ê¹Šê³  ë³µí•©ì ì¸ ë§›ì„ ê°–ê³  ìžˆì–´, ì§„í•œ ì†ŒìŠ¤ê°€ ì–´ìš°ëŸ¬ì§„ ìŠ¤í…Œì´í¬ë‚˜ ì–‘ê³ ê¸° ìš”ë¦¬ì™€ ìž˜ ì–´ìš¸ë¦½ë‹ˆë‹¤. ë˜í•œ, ê·¸ë¦´ì— êµ¬ìš´ ì±„ì†Œë‚˜ êµ¬ìš´ ë²„ì„¯ ê°™ì€ í’ë¯¸ê°€ ê°•í•œ ì±„ì†Œì™€ë„ í›Œë¥­í•œ ì¡°í™”ë¥¼ ì´ë£¹ë‹ˆë‹¤.\\\\n\\\\n**ì¶”ì²œ ì´ìœ **: \\\\r\\\\nì•„ë§ˆë¡œë„¤ëŠ” ê°•í•œ ë°”ë””ì™€ í’ë¶€í•œ íƒ€ë‹Œì„ ê°€ì§€ê³  ìžˆì–´, ê¸°ë¦„ì§„ ê³ ê¸° ìš”ë¦¬ì™€ í•¨ê»˜í•  ë•Œ ê·¸ ë§›ì„ ë”ìš± ë¶€ê°ì‹œí‚µë‹ˆë‹¤. ì´ ì™€ì¸ì˜ ì‚°ë„ëŠ” ìš”ë¦¬ì˜ ê¸°ë¦„ì§„ ë§›ì„ ìƒì‡„í•´ ì£¼ë©°, ê³¼ì¼ê³¼ í–¥ì‹ ë£Œì˜ ë³µí•©ì ì¸ í’ë¯¸ëŠ” ê³ ê¸°ì™€ ì±„ì†Œì˜ ë§›ì„ í•œì¸µ ë” ê¹Šê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\",\\n                    }\\n                ],\\n            },\\n        ],\\n        temperature=0,\\n        max_tokens=2048,\\n        top_p=1,\\n        frequency_penalty=0,\\n        presence_penalty=0,\\n        response_format={\"type\": \"text\"},\\n    )\\n    # return response\\n    return \"\".join([x.message.content for x in response.choices])\\n\\n\\nfrom langchain_core.runnables import RunnableLambda\\n\\n# íŒŒì¼ì„ ì‹¤í–‰í• ë•Œ  ì§ì ‘ì ìœ¼ë¡œ ì—¬ê¸° ì•ˆì— ìžˆëŠ”ê²ƒë“¤ì„ ì‚¬ìš©\\nif __name__ == \"__main__\":\\n    runnable = RunnableLambda(openai_runnable)\\n\\n    chain = runnable | StrOutputParser()\\n    response = chain.invoke(\"ì´ ì™€ì¸ê³¼ ì–´ìš¸ë¦¬ëŠ” ìŒì‹ì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”\")\\n'),\n",
       " Document(metadata={'source': '..\\\\ë³„ë„\\\\2.Streamlitì„ í™œìš©í•œ í”„ë¡œì íŠ¸2-wine_paring\\\\retrieval.py'}, page_content='')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "\n",
    "# í˜„ìž¬í´ë”(.) ì˜ .py íŒŒì¼ì„ ëª¨ë‘ ì¡°íšŒí•˜ì—¬ PythonLoader ë¡œ ë¡œë“œ\n",
    "loader = DirectoryLoader(\"../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "docs = loader.load()\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
